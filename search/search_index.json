{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Simple, Scalable &amp; Reusable EC2 Runners for GitHub Actions","text":"<p>This Action enables you to run a pool of self-hosted EC2 runners directly within the GitHub Actions runtime. Resource pooling, scale-in/scale-out, and the entire runner lifecycle are all managed by the action itself - no external control plane or infrastructure-as-code required.</p>"},{"location":"#motivation-yaml-first-control","title":"Motivation: YAML-first control","text":"<p>This action was explicitly designed to embed the control plane within GitHub Actions, avoiding the complexity of separate infrastructure or specialized expertise in Kubernetes or Terraform. While existing solutions either require external control planes or sacrifice performance for simplicity, this project combines the best of both worlds.</p> <p>The key benefits of this embedded approach are:</p> <ul> <li>Zero-Infrastructure Control Plane: No deploying and managing separate services or webhooks.</li> <li>Integrated Logging: Runner logs are readily accessible within your workflow run logs.</li> <li>Resource Pooling: Workflows first try to reuse warm, idle runners from a shared pool, minimizing cold-starts and optimizing costs.</li> </ul>"},{"location":"#how-it-works-an-embedded-lifecycle-manager","title":"How It Works: An Embedded Lifecycle Manager","text":"<p>This action operates using three distinct modes (<code>provision</code>, <code>release</code>, <code>refresh</code>) that you call from different jobs within your own workflows. This allows you to control, share, and scale runners directly from your YAML files.</p> <p></p> <ul> <li>The <code>provision</code> step is called at the start of a workflow to acquire runners.</li> <li>Your jobs run on the newly provisioned runners.</li> <li>The <code>release</code> step is called at the end to return the runners to the pool.</li> <li>A separate, scheduled workflow uses the <code>refresh</code> mode to perform system-wide maintenance.</li> </ul> <p>This model provides a powerful, YAML-centric approach to runner management, inspired by tools like <code>actions-runner-controller</code> and <code>terraform-aws-github-runner</code>.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to try it out? Follow our step-by-step guides to get up and running in minutes.</p> <p>Prerequisites Quickstart Advanced Configuration</p>"},{"location":"#digging-deeper","title":"Digging Deeper","text":"<p>For a more detailed look at the internal design and advanced use cases:</p> <ul> <li> Architectural Overview: Learn how the different components work together in our How It Works guide.</li> <li> Workflow Examples: See complete, practical examples for different CI/CD scenarios in our Examples section. (See advanced examples too)</li> </ul>"},{"location":"architecture/instance-initialization/","title":"Instance Agent Lifecycle","text":"<p>Every EC2 runner instance runs a persistent, state-driven agent responsible for its entire lifecycle. This agent allows an instance to be safely reused across multiple, distinct CI workflows by observing state changes in DynamoDB. It transitions between states like \"Waiting for Assignment\" and \"Executing Job\" based on commands from the control plane, making each runner a reusable, warm asset.</p>"},{"location":"architecture/instance-initialization/#one-time-initialization","title":"One-Time Initialization","text":"<p>Upon first boot, the agent performs a brief, one-time setup sequence:</p> <ul> <li>It configures logging and fetches necessary metadata from the EC2 environment.</li> <li>It executes an operator-provided pre-run script. This allows for custom setup like installing dependencies or warming caches before the runner is used. The success or failure of this script is signaled back to the control plane.</li> <li>It downloads the GitHub Actions runner software.</li> <li>Finally, it launches its background agents and enters the main reusability lifecycle (registration loop).</li> </ul>"},{"location":"architecture/instance-initialization/#parallel-support-agents","title":"Parallel Support Agents","text":"<p>Two independent agents run in parallel to the main lifecycle, providing continuous health monitoring and a safety net:</p> <ul> <li>Heartbeat Agent: This agent periodically writes a \"PING\" record to DynamoDB with a timestamp. This allows the control plane to continuously monitor the instance's liveness.</li> <li>Self-Termination Agent: This agent periodically checks the instance's <code>threshold</code> timestamp in its DynamoDB record. If the current time exceeds this deadline, the agent will command the EC2 instance to terminate itself. This acts as a fail-safe against orphaned or runaway instances, helping to control costs.</li> </ul>"},{"location":"architecture/instance-initialization/#the-reusability-lifecycle-a-coordinated-sequence","title":"The Reusability Lifecycle: A Coordinated Sequence","text":"<p>After initialization, the agent enters its main reusability loop. This lifecycle is a coordinated sequence of interactions between the instance, the control plane (acting via DynamoDB), and GitHub, allowing the instance to handle multiple jobs securely.</p> <pre><code>sequenceDiagram\n    participant CP as \"Control Plane\"\n    participant DDB as \"DynamoDB\"\n    participant Agent as \"Instance Agent\"\n    participant GH as \"GitHub\"\n\n    loop Reusability Cycle\n        %% Part 1: Wait for Assignment and Register %%\n        Note over Agent, DDB: Agent is in a&lt;br&gt;\"Waiting for Assignment\" state.\n        CP-&gt;&gt;DDB: Assigns runId to instance record&lt;br&gt;(when claimed or created)\n        Agent-&gt;&gt;+DDB: Polls for runId\n        DDB--&gt;&gt;-Agent: Returns runId\n\n        Agent-&gt;&gt;+GH: Register runner with runId as label\n        GH--&gt;&gt;-Agent: Registration successful\n        Agent-&gt;&gt;DDB: Write UD_REG_OK signal\n\n        %% Part 2: Execute Job and Wait for Release %%\n        Note over Agent, GH: Runner is now active&lt;br&gt;executing jobs for this runId \u26a1\n        Note over Agent, DDB: Agent now waits for release signal.\n        CP-&gt;&gt;DDB: Clears runId from instance record&lt;br&gt;(when released)\n\n        Agent-&gt;&gt;+DDB: Polls for runId change\n        DDB--&gt;&gt;-Agent: Returns empty runId\n\n        %% Part 3: Deregister and Cleanup %%\n        Agent-&gt;&gt;+GH: Deregister runner API call\n        GH--&gt;&gt;-Agent: Deregistration OK\n        Agent-&gt;&gt;DDB: Write completion signal\n        Note over Agent: Agent returns to \"Waiting for Assignment\" state \u2b06\ufe0f\n    end</code></pre> <p>The lifecycle shown in the diagram consists of three main phases:</p> <ol> <li>Wait for Assignment &amp; Register: The agent polls DynamoDB until the control plane assigns it a <code>runId</code>. It then uses this ID to register with GitHub.</li> <li>Execute &amp; Wait for Release: The runner is active and executing jobs. The agent concurrently polls DynamoDB, waiting for the control plane to signal completion by clearing the <code>runId</code>.</li> <li>Deregister &amp; Cleanup: Once the <code>runId</code> is cleared, the agent deregisters from GitHub, signals its completion, and returns to the beginning of the loop, ready for a new assignment.</li> </ol>"},{"location":"architecture/instance-initialization/#architectural-decisions","title":"Architectural Decisions","text":"<ul> <li>Indirect Signalling via DynamoDB: The agent communicates its status (e.g., <code>UD_REG_OK</code>) back to the control plane by writing signals to its DynamoDB record. This decoupled, asynchronous communication is fundamental to the system's design.</li> <li>Job Isolation with <code>runId</code> Labeling: Using the unique <code>runId</code> as the runner's sole label is a critical feature. It guarantees that jobs from one workflow can only land on a runner that has been specifically provisioned or claimed for it.</li> </ul>"},{"location":"architecture/lifecycle-walkthrough/","title":"End-to-End Lifecycle Walkthrough","text":"<p>This document tells the story of a single EC2 runner instance, showing how it moves through the system from creation to reuse and eventual termination. It illustrates how the core architectural concepts like <code>runId</code>, <code>threshold</code>, and indirect signaling work together in practice.</p>"},{"location":"architecture/lifecycle-walkthrough/#creation-of-an-instance","title":"Creation of an Instance","text":"<p>Imagine a workflow kicks off, requiring compute resources to execute CI jobs. Via <code>provision</code>, the controlplane first looks to reuse resources and checks the resource pool (backed by SQS queues) for idle instances.</p> <ul> <li>If a suitable instance is found, it gets immediately claimed.</li> <li>If not, the controlplane creates a new EC2 instance.</li> </ul> <p>As soon as an instance is created, it enters the created state in our central database (e.g., DynamoDB):</p> <pre><code>// new record in DB\n{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"created\",\n  \"runId\": \"run-7890\",\n  \"threshold\": \"2025-05-31T12:00:00Z\" // timeout for 'created' state (details below)\n}\n</code></pre> <p>What is <code>runId</code> and what is it for?</p> <p><code>runId</code> is the id that Github uniquely assigns to the workflow and is part of the Github Context. We use this to uniquely associate this instance with the workflow's CI jobs, ensuring it only runs the intended jobs.</p> <p>What is <code>threshold</code> for?</p> <p><code>threshold</code> here defines a timeout for how long this instance can remain in a specific state; we\u2019ll explain this fully in the Expiration section below.</p>"},{"location":"architecture/lifecycle-walkthrough/#initialization-and-indirect-signaling","title":"Initialization and Indirect Signaling","text":"<p>After creation, the instance begins initializing itself. Since the controlplane and the instances cannot communicate directly, they use indirect signaling through a shared database.</p> <p>The instance performs two essential initialization steps:</p> <ul> <li>Pre-runner Script: Runs the user-defined script on the instance.</li> <li>Runner Registration: The instance registers itself as a GitHub Actions runner using the <code>runId</code>.</li> </ul> <p>Once both steps complete successfully, the instance signals readiness, and the controlplane updates the state to running in the database:</p> <pre><code>// state created-&gt;running, new threshold assigned\n{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"running\", \n  \"runId\": \"run-7890\",\n  \"threshold\": \"2025-05-31T12:10:00Z\"\n}\n</code></pre> ~A Closer Look ~ Indirectly Signaling that the Instance is Ready  <p>Due to network and security constraints - the controlplane can\u2019t communicate directly with the instances. They rely instead on regularly reading and writing their state in a shared central database (DynamoDB). Below is a lower level view of a successful registration. <pre><code>sequenceDiagram\n    participant Controlplane as \"Provision (Controlplane)\"\n    participant DynamoDB as \"State Store\"\n    participant Instance\n    participant Github\n\n    Note over Controlplane, Instance: Controlplane creates instance with AWS\n    Controlplane-&gt;&gt;DynamoDB: Immediately register (state: created, runId)\n    Instance-&gt;&gt;Instance: Initialize (pre-runners-script)\n    Instance-&gt;&gt;Github: Register with runId\n    Instance--&gt;Github: Ready to pickup CI jobs with runId\n\n    Controlplane-&gt;&gt;+DynamoDB: Periodically Read/Monitor State\n    Instance-&gt;&gt;DynamoDB: Signal Readiness \u2705\n    DynamoDB-&gt;&gt;-Controlplane: Found Readiness Signal \u2705 \n\n    Controlplane-&gt;&gt;DynamoDB: Transition (created-&gt;running)\n    Note over Controlplane: Finish provision</code></pre></p>"},{"location":"architecture/lifecycle-walkthrough/#running-ci-jobs","title":"Running CI Jobs","text":"<p>With initialization complete, the instance is now ready to execute CI jobs. Your GitHub Actions workflow specifically targets this instance by referencing the workflow\u2019s unique ID (runId) in its job definition:</p> <pre><code>runs-on: ${{ github.run_id }}\n</code></pre> <p>Since the instance registered itself with exactly this <code>runId</code>, it guarantees that these jobs run only on the correct, assigned instance. Jobs run smoothly without interference from other workflows.</p>"},{"location":"architecture/lifecycle-walkthrough/#releasing-an-instance-back-to-the-resource-pool","title":"Releasing an Instance Back to the Resource Pool","text":"<p>When all CI jobs finish, the workflow executes <code>release</code>. This is tasked with safely placing the runner to the resource pool - enabling reuse.</p> <p>The release component ensures the instance is safely reset and ready for future workflows. Behind the scenes, the controlplane and the runner instance coordinate via the shared state store to facilitate a clean transition. This coordination includes clearing the <code>runId</code>, safely deregistering from GitHub Actions, and confirming readiness for reuse.</p> <p>At a lower level, the responsibilities of the controlplane and instance are as follows:</p> <ul> <li>Controlplane: Updates instance state, monitors signaling, places instance back into pool.</li> <li>Instance: Detects state change, deregisters from GitHub Actions, signals completion.</li> </ul> <p>Here\u2019s how this transition appears in the database:</p> <pre><code>// state running-&gt;idle, runId: \"run-7890\"-&gt;\"\", new threshold assigned\n{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"idle\",\n  \"runId\": \"\", \n  \"threshold\": \"2025-05-31T12:20:00Z\"\n}\n</code></pre> <p>The instance is now in the resource pool and ready for another workflow.</p> What is placed in the resource pool?  <p><pre><code>// Distilled version\n{\n  \"instanceId\": \"i-123456\",\n  \"usageClass\": \"on-demand\",\n  \"instanceType\": \"c6i.large\",\n}\n</code></pre> Remember, the resource pool is simply a collection of SQS queues. To see how this is used, see the next section.</p> ~A Closer Look ~ How is the Instance Deregistered and placed in the Resource Pool?  <pre><code>sequenceDiagram\n    participant Controlplane as \"Release (Controlplane)\"\n    participant DynamoDB as \"State Store\"\n    participant Instance\n    participant GitHub\n\n    Note over Controlplane: Release triggered after CI jobs\n    Controlplane-&gt;&gt;DynamoDB: Scan instances with the workflows run_id\n    Note over Controlplane: Found instance id/s under run_id \n\n    Controlplane-&gt;&gt;DynamoDB: release instance (state: running-&gt;idle, runId: '')\n    Note over Controlplane, Instance: Instances sees emptied runId, initiates deregistration\n\n    Controlplane-&gt;&gt;+DynamoDB: Look for deregistration signal\n\n    Instance-&gt;&gt;GitHub: Deregister runner from Github\n    GitHub--&gt;Instance: No more active session with Github \u2705\n    Instance-&gt;&gt;DynamoDB: Signal successful deregistration \u2705\n    DynamoDB-&gt;&gt;-Controlplane: Successful Deregistration signal found \u2705\n    Note over Controlplane: Add instance to resource pool\n    Note over Controlplane: Release concludes \n    Instance--&gt;DynamoDB: Looking for new assigned runId \u267b\ufe0f</code></pre>"},{"location":"architecture/lifecycle-walkthrough/#reusing-instances-selection-claiming","title":"Reusing Instances (Selection &amp; Claiming)","text":"<p>With the released instance now available in the resource pool, let\u2019s imagine another workflow triggers requesting compute resources. The controlplane first consults the resource pool to check if existing idle resources match the workflow\u2019s requirements.</p> <p>The controlplane evaluates key attributes from resource pool messages, some of these include:</p> <ul> <li>usageClass (spot or on-demand) to align with cost or availability needs.</li> <li>instanceType to satisfy performance constraints.</li> </ul> <p>For example, a workflow request might look like:</p> <pre><code># provision inputs\nwith:\n  usage-class: on-demand\n  allowed-instance-types: \"c*\" # Matches any instance type starting with 'c'\n</code></pre> <p>Matching against our resource pool entry:</p> <pre><code>{\n  \"instanceId\": \"i-123456\",\n  \"usageClass\": \"on-demand\",\n  \"instanceType\": \"c6i.large\"\n}\n</code></pre> <p>Since our previously released instance <code>i-123456</code> matches these requirements exactly, the controlplane attempts to claim <code>i-12346</code>.</p> <p>If the claim is successful (no other workflow has claimed it first):</p> <pre><code>// state idle-&gt;claimed, runId: \"\"-&gt;\"run-9999\", new threshold assigned\n{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"claimed\",\n  \"runId\": \"run-9999\",\n  \"threshold\": \"2025-05-31T12:30:00Z\"\n}\n</code></pre> <p>Racing against other Workflows </p> <p>Claims might fail if a race condition occurs (another workflow claiming simultaneously). In such cases, the controlplane either selects another idle instance or provisions a new one.</p> <p>After successful claiming, the instance detects the new <code>runId</code> (ie. <code>run-9999</code>) and registers itself with GitHub with this label. Shortly thereafter, the controlplane transitions the instance from <code>claimed</code> to <code>running</code>, indicating it is now ready to execute CI jobs.</p> ~A Closer Look ~ How is an instance claimed and how do we know it's ready?  <pre><code>sequenceDiagram\n    participant Controlplane as \"Provision (Controlplane)\"\n    participant DynamoDB as \"State Store\"\n    participant Instance\n    participant Github\n\n    Note over Controlplane: Fetch instance from pool\n    Controlplane-&gt;&gt;Controlplane: Instance deemed valid\n\n    Controlplane-&gt;&gt;DynamoDB: claim instance (state: idle-&gt;claimed, runId: new_run_id)\n    Note over Controlplane, Instance: If claim suceeds then new run_id is given \u26a1\ufe0f instance detects new_run_id via db\n    Controlplane-&gt;&gt;+DynamoDB: Monitor for registration signal\n\n    Instance-&gt;&gt;Github: register with new_run_id\n    Instance-&gt;&gt;DynamoDB: send registration signal \u2705\n    Instance--&gt;Github: pickup any ci jobs \u267b\ufe0f\n    DynamoDB--&gt;&gt;-Controlplane: registration signal found \u2705\n\n    Controlplane-&gt;&gt;DynamoDB: State Transition (state: claimed-&gt;running)\n    Note over Controlplane: Completes if compute is fulfilled</code></pre> <p>This reuse cycle repeats smoothly as long as instances remain healthy, continue matching workflow requirements, and remain within configured operational lifetimes.</p> Requiring more instances than the pool? <p>Say that the workflow request requires more resources than what the pool can provide? See request: <pre><code># provision inputs\nwith:\n  instance-count: 10 # &lt;---\n  usage-class: on-demand\n  allowed-instance-types: \"c*\"\n</code></pre> This is expected, the controlplane simply creates new resources to satisfy the requirements. After successful creation, selected and created instances are transtitioned to <code>running</code>.</p>"},{"location":"architecture/lifecycle-walkthrough/#expiration-thresholds-and-termination","title":"Expiration, Thresholds, and Termination","text":"<p>If, for some reason, instances from getting stuck or running longer than intended, each state (created, running, idle) carries a <code>threshold</code> - this is a timestamp indicating how long an instance may safely remain in its current state. If the instance surpasses this timestamp, it\u2019s considered expired:</p> <pre><code>{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"running\",\n  \"runId\": \"run-7890\",\n  \"threshold\": \"2025-05-31T12:10:00Z\" // timestamp expired\n}\n</code></pre> <p>When the refresh worker which executes via cron sees an expired instance, it issues a termination command directly to AWS (TerminateInstances API call) and transitions it to terminated.</p> <pre><code>{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"terminated\",\n  \"runId\": \"\",\n  \"threshold\": \"\"\n}\n</code></pre> <p>For redundancy, the instance itself observes its own lifetime. If it sees that it has expired, it and issues a termination command directly to AWS to terminate itself.</p> ~A Closer Look ~ When does the Refresh Worker terminate expired instance/s  <pre><code>sequenceDiagram\n    participant Refresh_Worker as \"Refresh (Controlplane)\"\n    participant DynamoDB as \"State Store\"\n    participant AWS\n\n    Note over Refresh_Worker: Triggered by CRON \u23f1\ufe0f\n    Refresh_Worker-&gt;&gt;+DynamoDB: Scan for instances (check expired threshold)\n    DynamoDB--&gt;&gt;-Refresh_Worker: Returns expired instance ids\n    Refresh_Worker-&gt;&gt;AWS: TerminateInstances(instance_ids)\n    Note over AWS: Instance/s is terminated \ud83e\udea6\n    Refresh_Worker-&gt;&gt;+DynamoDB: Update instances to terminated (state: 'terminated', runId: '', threshold: '')</code></pre> ~A Closer Look ~ How does the Instance carry out self-termination?  <pre><code>sequenceDiagram\n    participant Instance\n    participant DynamoDB as \"State Store\"\n    participant AWS\n\n    Note over Instance: Background Process in Instance \n    Instance--&gt;DynamoDB: periodically fetch own threshold \u267b\ufe0f\n    Note over Instance: Compares threshold against local time\n    Instance-&gt;&gt;Instance: determine self as expired\n    Instance-&gt;&gt;AWS: TerminateInstances(instance_id)\n    AWS-&gt;&gt;Instance: AWS terminates instance\n    Note over Instance: shutdown</code></pre> <p>These mechanisms cleans up expired resources. They ensure the infrastructure remains healthy and efficient by automatically cleaning up unused/problematic instances.</p>"},{"location":"architecture/overview/","title":"Detailed Design","text":""},{"location":"architecture/overview/#detailed-architecture-overview","title":"Detailed Architecture Overview","text":"<p>Below is the detailed architecture diagram showcasing all components:</p> <p></p> <p>The core components in more detail:</p> <ul> <li>Controlplane: Orchestrates instance management via distinct operational modes.</li> <li>Resource Pool (SQS Queue): Manages idle instances ready for reuse.</li> <li>Central State Store (DynamoDB): Maintains current instance state, signaling indirectly between controlplane and instances.</li> <li>Instances/Runners: EC2 machines executing the actual CI jobs.</li> </ul>"},{"location":"architecture/overview/#core-concepts-expanded","title":"Core Concepts Expanded","text":""},{"location":"architecture/overview/#controlplane-operational-modes","title":"Controlplane Operational Modes","text":"<p>The controlplane operates in three distinct modes:</p> <ul> <li>Provision: Acquires EC2 resources either by claiming from the resource pool or provisioning new instances.</li> <li>Release: Returns instances to the resource pool after workflows complete, resetting instance state.</li> <li>Refresh: Periodically validates instance health and lifecycle thresholds, initiating termination of unhealthy or expired instances.</li> </ul> <p>Controlplane modes visualized</p> <pre><code>graph TD\n    A[Workflow Starts \u26a1\ufe0f] --&gt; B(Provision Mode);\n    B --&gt; C[Instance Ready for CI];\n\n    D[Workflow Completes \u26a1\ufe0f] --&gt; E(Release Mode);\n    E --&gt; F[Instance Returned to Pool];\n\n    G[Manual \ud83d\udc48/Cron \u23f1\ufe0f] --&gt; H(Refresh Mode);\n    H --&gt; I[Initialization/Maintenance];</code></pre> <p>These modes interact through a shared state mechanism stored in DynamoDB, enabling indirect signaling between runners and the controlplane.</p>"},{"location":"architecture/overview/#instances-and-runners","title":"Instances and Runners","text":"<p>EC2 Instances (as self-hosted runners) are dynamically managed resources executing CI workflows. They maintain minimal agents installed at instance startup for:</p> <ul> <li>Initializing runner environments via custom scripts (<code>pre-runner-script</code>).</li> <li>Registering and deregistering with GitHub Actions runner APIs.</li> <li>Sending periodic health signals (heartbeats) to the shared state store.</li> <li>Observing state changes (e.g. runId assignments) for timely registration.</li> </ul>"},{"location":"architecture/overview/#instance-states-and-transition-mechanisms","title":"Instance States and Transition Mechanisms","text":"<p>Each instance has a clearly defined lifecycle managed through distinct states stored in DynamoDB:</p> State Technical Definition created Instance created; initialization pending (waiting on runner registration &amp; scripts). running Runner initialized and actively accepting CI jobs. idle Instance healthy and available in the resource pool for reuse. claimed Instance reserved by a workflow, undergoing final validation checks. terminated Instance terminated following expiration, unhealthy status, or explicit shutdown. <p>These states allow the controlplane to track and manage instances seamlessly.</p> <p>Detailed State Transition Diagram</p> <p></p> <p>The transitions between states are triggered by specific events in the system:</p> From State To State Triggering Event <code>created</code> <code>running</code> The instance agent successfully initializes and registers. <code>running</code> <code>idle</code> The workflow completes, and the <code>release</code> process begins. <code>idle</code> <code>claimed</code> The <code>provision</code> process selects the instance from the pool for a new workflow. <code>claimed</code> <code>idle</code> The instance fails a health/registration check after being claimed and is returned to the pool. <code>(any)</code> <code>terminated</code> The instance's <code>threshold</code> and is transitioned by the <code>refresh</code> process. <p>This state management gives the controlplane ability to reuse runners effectively and terminate them automatically when no longer viable.</p>"},{"location":"architecture/overview/#lifecycle-of-an-instance","title":"Lifecycle of an Instance","text":"<p>To understand how these components and concepts work together, read our End-to-End Lifecycle Walkthrough.</p>"},{"location":"architecture/overview/#technical-deep-dives","title":"Technical Deep Dives","text":"<p>For a detailed exploration of individual components, their challenges, and solutions, see:</p> <ul> <li>Provision: Instance reuse/creation, selection logic, resource matching, and AWS API interactions.</li> <li>Release: Safe instance deregistration and resource pool placement.</li> <li>Refresh: Controlplane initialization, maintenance, and periodic checks for safe termination of long-running instances.</li> <li>Resource Pool: SQS-backed resource pool, message structure, and producer/consumer roles.</li> <li>Instance Initialization: Instance agent managing startup, GitHub registration/deregistration loop, heartbeats, and safe self-termination.</li> </ul> <p></p>"},{"location":"architecture/refresh/","title":"Refresh: System Initialization &amp; Maintenance \u2699\ufe0f","text":"<p>The Refresh process is a periodic, idempotent background operation, executed by a scheduled GitHub Actions workflow (e.g., via <code>cron</code>), that acts as the system's central initializer and maintainer. Unlike the event-driven <code>provision</code> and <code>release</code> modes which are tied to workflows, <code>refresh</code> operates on the entire system to ensure the live infrastructure and configuration in AWS align with a centrally defined desired state.</p> <p>Its core purpose is to initialize foundational infrastructure and to perform routine maintenance.</p>"},{"location":"architecture/refresh/#the-refresh-model-desired-vs-actual-state","title":"The Refresh Model: Desired vs. Actual State","text":"<p>The entire Refresh process is built on this model: it compares a Desired State (the configuration you provide) with the Actual State (the resources that exist in AWS) and performs the necessary actions to align them.</p> <ul> <li>Desired State: The configuration you provide as input. This includes the AMI for runners, resource class definitions, maximum instance lifetimes, and approved subnets. This is your \"source of truth.\"</li> <li>Actual State: The resources and records that currently exist, such as EC2 Launch Templates, SQS queues, and instance state records in DynamoDB.</li> </ul> <p>This model makes the process inherently idempotent, it can be run repeatedly with the same desired state, and it will only make changes if a drift is detected.</p>"},{"location":"architecture/refresh/#core-refresh-tasks","title":"Core Refresh Tasks","text":"<p>The Refresh process performs two primary architectural functions: initialization and maintenance.</p>"},{"location":"architecture/refresh/#1-system-initialization","title":"1. System Initialization \ud83c\udfd7\ufe0f","text":"<p>This task handles the setup and configuration of foundational AWS resources. It ensures the system has the necessary infrastructure and parameters in place before the <code>provision</code> and <code>release</code> processes need them. Each row in the table represents an independent initialization task.</p> Component Desired State (Input) Initialization Action Architectural Purpose \ud83d\ude80 EC2 Launch Template AMI ID, IAM Profile, User Data script, etc. Creates or updates the EC2 Launch Template. Stores its ID in DynamoDB. Decouples instance <code>provisioning</code> from the details of instance configuration. \ud83d\udce5 Resource Pools A list of <code>resourceClass</code> definitions. For each <code>resourceClass</code>, it ensures a corresponding SQS queue exists. Stores the queue URLs in DynamoDB. Provides a discoverable, central endpoint for the idle runner pools. \ud83d\udd27 Operational Parameters Values for <code>max-idle-time</code>, <code>subnet-ids</code>, etc. Updates the corresponding key-value records in DynamoDB. Allows for dynamic, centralized tuning of the system's operational behavior. \ud83d\udd11 GitHub Auth A GitHub PAT with <code>repo</code> scope. Periodically generates a new, short-lived GitHub Actions registration token and stores it in DynamoDB. Enhances security by ensuring new instances register with a temporary token."},{"location":"architecture/refresh/#2-instance-lifecycle-maintenance","title":"2. Instance Lifecycle Maintenance \ud83d\uddd1\ufe0f","text":"<p>This task acts as the system's garbage collector, enforcing the defined lifecycle rules to prevent orphaned or expired resources.</p> <ul> <li>Identifying Expired Instances: The primary policy is the <code>threshold</code> timestamp on each instance record in DynamoDB. The maintainer scans for any <code>idle</code>, <code>claimed</code>, or <code>running</code> instances that have lived past their expiration time.</li> <li>Resilient Termination Process: To prevent race conditions, it uses a safe, two-phase commit process:<ol> <li>Phase 1: Mark for Termination (in DynamoDB): It first attempts to \"lock\" the instance by changing its state to <code>terminated</code>.</li> <li>Phase 2: Terminate Instance (in EC2): Only after an instance is successfully marked in the database does it issue the <code>TerminateInstances</code> command to AWS.</li> </ol> </li> <li>Artifact Cleanup: After confirming termination, it removes associated data for the terminated instances (like heartbeat records) from DynamoDB.</li> </ul> <p>Sequence Diagram: Resilient Termination</p> <pre><code>    sequenceDiagram\n        participant Maintainer as \"Refresh Maintainer\"\n        participant DDB as \"DynamoDB\"\n        participant EC2\n\n        Maintainer-&gt;&gt;+DDB: Find instances where state is 'idle'/'running'&lt;br&gt;and threshold is expired\n        DDB--&gt;&gt;-Maintainer: Return list of expired instances\n\n        loop For each expired instance\n            Maintainer-&gt;&gt;+DDB: Conditionally update state to 'terminated'\n            DDB--&gt;&gt;-Maintainer: Update OK\n        end\n        Note right of Maintainer: Now we have a locked list&lt;br&gt;of instances to terminate.\n\n        Maintainer-&gt;&gt;+EC2: Terminate instances by ID\n        EC2--&gt;&gt;-Maintainer: Termination initiated\n\n        Maintainer-&gt;&gt;+DDB: Delete associated artifacts&lt;br&gt;(heartbeats, worker signals)\n        DDB--&gt;&gt;-Maintainer: Cleanup OK</code></pre>"},{"location":"architecture/refresh/#centralized-config-decoupling","title":"Centralized Config &amp; Decoupling \u2728","text":"<p>This centralized <code>refresh</code> approach provides advantages to decoupling:</p> <ul> <li>Centralized Configuration: All major changes to the runner environment (like updating the AMI) can be done by changing a single configuration value and letting the initializer handle the rollout.</li> <li>Decoupling of Concerns: The <code>provision</code> logic doesn't need to know how to build an instance; it just needs to know which Launch Template to use. The <code>refresh</code> process handles the \"how,\" keeping concerns cleanly separated.</li> </ul> <p></p>"},{"location":"architecture/release/","title":"Release Process: Returning Runners to the Pool","text":"<p>The release process ensures that the EC2 runner instances are safely returned to the resource pool once their assigned CI jobs are complete in order to be potentially reused.</p>"},{"location":"architecture/release/#the-core-design-an-asynchronous-handshake","title":"The Core Design: An Asynchronous Handshake \ud83e\udd1d","text":"<p>The release process is built on an asynchronous handshake mediated by DynamoDB. The Control Plane and the EC2 instance coordinate their actions by observing and modifying a shared state record.</p> <p>Sequence Diagram: Successful Release</p> <pre><code>sequenceDiagram\n    participant CP as \"Release&lt;br&gt;(Control Plane)\"\n    participant DDB as \"DynamoDB\"\n    participant Instance\n\n    Note right of CP: Workflow completion triggers release \u26a1\n    CP-&gt;&gt;+DDB: Update instance state: set to 'idle', clear 'runId'\n    DDB--&gt;&gt;-CP: Update successful\n    CP-&gt;&gt;+DDB: Polls for 'UD_REMOVE_REG_OK' signal\n\n    Instance-&gt;&gt;+DDB: Polls its record, detects 'runId' is cleared\n    DDB--&gt;&gt;-Instance: State change confirmed\n\n    Note over Instance: Conduct deregistration from Github\n\n    Instance-&gt;&gt;DDB: Write 'UD_REMOVE_REG_OK' signal&lt;br&gt;on successful deregistration \ud83d\udc4c\n\n    DDB--&gt;&gt;-CP: Signal found \ud83d\udc4c\n\n    Instance--&gt;DDB: Polls its record for&lt;br&gt;new runId to register against \u267b\ufe0f\n    Note right of CP: Instance mesage sent to pool \ud83d\udc4c</code></pre>"},{"location":"architecture/release/#the-release-lifecycle-step-by-step","title":"The Release Lifecycle: Step-by-Step","text":"<p>The entire lifecycle revolves around this coordinated state-driven handshake.</p>"},{"location":"architecture/release/#1-initiation-control-plane-identifies-runners","title":"1. Initiation: Control Plane Identifies Runners","text":"<p>The process begins when a GitHub Actions workflow finishes, triggering the Control Plane to start the release for all runners associated with that workflow's <code>runId</code>.</p>"},{"location":"architecture/release/#2-the-trigger-the-runid-is-cleared","title":"2. The Trigger: The <code>runId</code> is Cleared","text":"<p>This is the most critical step. For each <code>running</code> instance, the Control Plane performs an atomic update on its record in DynamoDB. It sets the <code>state</code> to <code>idle</code> and, most importantly, clears the <code>runId</code>. This change is the key signal that the instance is no longer needed for its current job and must begin the release process.</p> <p>BEFORE (running state):</p> <pre><code>{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"running\",\n  \"runId\": \"run-9999\",\n  \"threshold\": \"2025-05-31T12:30:00Z\"\n}\n</code></pre> <p>AFTER (idle state trigger):</p> <pre><code>{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"idle\",\n  \"runId\": \"\", // This is the trigger\n  \"threshold\": \"2025-05-31T12:50:00Z\"\n}\n</code></pre>"},{"location":"architecture/release/#3-reaction-the-instance-deregisters-itself","title":"3. Reaction: The Instance Deregisters Itself","text":"<p>The agent running on the EC2 instance constantly monitors its own record in DynamoDB. When it detects that the <code>runId</code> has been cleared, it initiates its self-cleanup procedure. Overall, deregistration prevents GitHub from attempting to send new jobs to an instance that is being retired.</p>"},{"location":"architecture/release/#4-confirmation-the-instance-signals-readiness","title":"4. Confirmation: The Instance Signals Readiness","text":"<p>After successfully deregistering from GitHub and completing its cleanup, the instance agent writes a confirmation signal (e.g., <code>UD_REMOVE_REG_OK</code>) back to its DynamoDB record. This signal serves as a receipt, informing the Control Plane that the instance has completed its responsibilities and is ready to be pooled.</p>"},{"location":"architecture/release/#5-completion-the-control-plane-pools-the-runner","title":"5. Completion: The Control Plane Pools the Runner","text":"<p>The Control Plane, which has been polling for the instance's confirmation signal, now verifies that the signal has been received. Once confirmed, it constructs a message containing the instance's details (ID, instance type, resource class, etc.) and sends it to the appropriate SQS queue. The instance is now officially in the resource pool.</p>"},{"location":"architecture/release/#failure-handling-and-resilience","title":"Failure Handling and Resilience","text":"<p>To account for instance failures, the Control Plane does not wait indefinitely for the instance's confirmation signal.</p> <p>If an instance fails to write its readiness signal to DynamoDB within a predefined timeout, the Control Plane assumes the instance is faulty or has crashed. Instead of adding a potentially corrupted runner to the pool, the Control Plane will mark the instance for termination. This ensures that only healthy, verified runners are reused, maintaining the overall health and reliability of the fleet.</p> <p>Sequence Diagram: Release Failure (Instance Timeout)</p> <pre><code>sequenceDiagram\n    participant CP as \"Release&lt;br&gt;(Control Plane)\"\n    participant DDB as \"DynamoDB\"\n    participant Instance\n\n    Note right of CP: Workflow completion triggers release \u26a1\n    CP-&gt;&gt;+DDB: Update instance state: set to 'idle', clear 'runId'\n    DDB--&gt;&gt;-CP: Update successful\n\n    Instance-&gt;&gt;+DDB: Polls its record, detects 'runId' is cleared\n    DDB--&gt;&gt;-Instance: State change confirmed\n    Note over Instance: Instance crashes or fails to&lt;br&gt;deregister from GitHub. It never&lt;br&gt;writes the confirmation signal. \u274c\n\n    CP-&gt;&gt;+DDB: Polls for 'UD_REMOVE_REG_OK' signal\n\n    loop For a defined timeout\n        DDB--&gt;&gt;CP: Signal not found...\n    end\n\n    DDB--&gt;&gt;-CP: Final response: Polling timed out.\n\n    Note right of CP: Instance is faulty. Marking for termination.\n    CP-&gt;&gt;DDB: Update instance record - expire threshold\n    Note right of DDB: Expired threshold later seen by&lt;br&gt;other components of controlplane&lt;br&gt;is terminated asynchronously\n    Note right of CP: Instance will NOT be sent to SQS pool.</code></pre>"},{"location":"architecture/release/#final-state-a-reusable-runner","title":"Final State: A Reusable Runner","text":"<p>The successful outcome of the release process is a message in an SQS queue. This message represents a clean, idle, and fully-vetted EC2 runner, ready to be picked up by the Provision Process for the next incoming workflow.</p> <p></p>"},{"location":"architecture/resource-pool/","title":"Resource Pool","text":"<p>The Resource Pool is a critical component responsible for holding information about idle EC2 runner instances that are ready for reuse by CI workflows. This mechanism is key to providing \"warm\" runners, reducing the latency associated with provisioning new instances from scratch.</p>"},{"location":"architecture/resource-pool/#implementation","title":"Implementation","text":"<p>The Resource Pool is implemented as a collection of SQS queues. Each distinct \"runner class\" (ie. based on cpu and memory) is partitioned to have its own dedicated SQS queue. This segregation ensures that workflows can request and receive instances that match their specific hardware requirements quickly.</p> <p>The definition and management of these SQS queues, including their creation and association with specific resource classes, are handled within refresh</p>"},{"location":"architecture/resource-pool/#interaction-with-the-pool","title":"Interaction with the Pool","text":"<p>Graph for Release and Reuse via Resource Pool</p> <pre><code>graph LR\n    subgraph Previous Workflow\n        A[Running Instance &lt;br&gt; state: running]\n    end\n\n    subgraph Resource Pool\n        B(SQS Resource Pool &lt;br&gt; idle runners)\n    end\n\n    subgraph Next Workflow\n        C[Claimed Instance &lt;br&gt; state: claimed]\n    end\n\n    A -- \"Release Process\" --&gt; B;\n    B -- \"Provision Process &lt;br&gt; (Pickup Manager)\" --&gt; C;</code></pre>"},{"location":"architecture/resource-pool/#producers-adding-instances-to-the-pool","title":"Producers (Adding Instances to the Pool)","text":"<p>When an EC2 runner instance completes its assigned CI jobs and is successfully processed by the Release mechanism, a message representing this now-idle instance is sent to the SQS queue corresponding to its resource class.</p>"},{"location":"architecture/resource-pool/#consumers-retrieving-instances-from-the-pool","title":"Consumers (Retrieving Instances from the Pool)","text":"<p>The primary consumer of the Resource Pool is the Pickup Manager, which operates during the Provision mode. The Pickup Manager dequeues messages, filters them, and passes suitable candidates to a requesting Claim Worker. (For more details, see Pickup Manager).</p>"},{"location":"architecture/resource-pool/#sqs-message-format","title":"SQS Message Format","text":"<p>When an instance is added to a resource pool queue, the SQS message payload contains details about the instance necessary for the Pickup Manager to evaluate its suitability for a workflow.</p> <p>Example Distilled Payload:</p> <pre><code>{\n  \"id\": \"i-123\",\n  \"resourceClass\": \"large\",\n  \"instanceType\": \"c6i.large\",\n  \"cpu\": 2,\n  \"mmem\": 4096,\n  \"usageClass\": \"on-demand\"\n}\n</code></pre> <p>Field Descriptions:</p> Field Type Purpose &amp; Notes <code>id</code> string The EC2 Instance ID. Primary key for subsequent DynamoDB lookups by the Claim Worker. <code>resourceClass</code> string The specific resource class this instance belongs to (e.g., \"medium-linux\"). Ensures the instance is in the correct pool. <code>instanceType</code> string The concrete EC2 instance type (e.g., \"c6i.large\"). Used for matching against workflow requirements. <code>cpu</code>/<code>mmem</code> number The number of vCPUs and minimum memory (in MiB) of the instance <code>usageClass</code> enum (<code>spot</code>|<code>on-demand</code>) Indicates if the instance is Spot or On-Demand. Must match the <code>usageClass</code> requested by the workflow. <p></p>"},{"location":"architecture/provision/post-provision/","title":"Post-Provision: Finalizing and Committing Resources","text":"<p>The Post-Provision phase concludes the provisioning operation. It evaluates the combined results from Instance Selection and Instance Creation to determine if the requested compute capacity was successfully acquired. Based on this outcome, it either commits the resources to the workflow or performs a graceful rollback.</p>"},{"location":"architecture/provision/post-provision/#1-overall-state-reconciliation","title":"1. Overall State Reconciliation","text":"<p>This initial step reconciles the results from the selection and creation phases to determine if the full request for healthy runners was met. The evaluation requires that all newly created instances passed their validation and that the total number of ready instances (selected + created) meets the workflow's demand. The outcome is a simple <code>'success'</code> or <code>'failure'</code> status, which dictates the next action.</p> <p>Sequence Diagram: Post-Provision Outcomes</p> <pre><code>sequenceDiagram\nparticipant Orchestrator as \"Post-Provision&lt;br&gt;Orchestrator\"\nparticipant DDB as \"DynamoDB\"\nparticipant SQS as \"SQS Pool\"\nparticipant AWS\n\nNote over Orchestrator: Reconcile status of selection/creation&lt;br&gt;overallStatus ('success' \ud83d\udc4c or 'failure' \ud83d\udeab)\n\nalt overallStatus is 'success' \ud83d\udc4c\n    Note right of Orchestrator: Path A: Commit Resources\n    Orchestrator-&gt;&gt;+DDB: Update all instances to 'running' + threshold\n    DDB--&gt;&gt;-Orchestrator: OK\nelse overallStatus is 'failure' \ud83d\udeab\n    Note right of Orchestrator: Path B: Graceful Rollback\n    Orchestrator-&gt;&gt;+DDB: Revert 'claimed' instances to 'idle'\n    DDB--&gt;&gt;-Orchestrator: OK\n    Orchestrator-&gt;&gt;SQS: Return instances to pool\nend\n\nrect\n    Note right of Orchestrator: Path C (Emergency Cleanup): If an unexpected error occurs,&lt;br&gt;all involved instances are terminated&lt;br&gt;and their DDB records are purged. This is the safety net.\n    Orchestrator-&gt;&gt;DDB: Delete instance records\n    Orchestrator-&gt;&gt;AWS: Sent termination signals to selected &amp; created instances\nend</code></pre>"},{"location":"architecture/provision/post-provision/#2-path-a-successful-provisioning","title":"2. Path A: Successful Provisioning","text":"<p>Taken when the overall status is <code>'success'</code>. This path transitions all acquired runners into an operational state for the workflow.</p> <ul> <li>Core Responsibility: Formally commit all acquired runners to the workflow.</li> <li>Key Actions:<ul> <li>Update State to <code>running</code>: Transitions all <code>claimed</code> (from selection). All created instances already <code>running</code> at this point (as per fleet validation)</li> <li>Set Operational Lifetime: Assigns a new lifetime threshold to each <code>running</code> instance, defining how long it can execute jobs before being considered for cleanup.</li> </ul> </li> </ul>"},{"location":"architecture/provision/post-provision/#3-path-b-unsuccessful-provisioning-graceful-rollback","title":"3. Path B: Unsuccessful Provisioning (Graceful Rollback)","text":"<p>Taken when the overall status is <code>'failure'</code> due to insufficient capacity. This path safely rolls back partially acquired resources to ensure the system remains clean and the workflow fails quickly.</p> <ul> <li>Core Responsibility: Return any <code>claimed</code> but now unused instances to the resource pool.</li> <li>Key Actions:<ul> <li>Release Claimed Instances: Any instances <code>claimed</code> from the pool are reverted to an <code>idle</code> state, disassociated from the workflow's <code>runId</code>, and returned to the SQS resource pool, making them available for other workflows.</li> <li>Report Failure: The overall provisioning attempt is marked as failed, surfacing a clear error to the GitHub Actions workflow.</li> <li>Note: Newly created instances that failed validation are assumed to be terminated by the previous phase - see fleet validation.</li> </ul> </li> </ul>"},{"location":"architecture/provision/post-provision/#4-path-c-unhandled-error-cleanup-aggressive-safety-net","title":"4. Path C: Unhandled Error Cleanup (Aggressive Safety Net)","text":"<p>This is a safety net. Invoked by an unexpected error during the post-provisioning logic itself. Its goal is to aggressively clean up all potentially involved resources to prevent orphaned instances or inconsistent data.</p> <ul> <li>Core Responsibility: Force-terminate all involved resources to guarantee a clean state after an unexpected error.</li> <li>Key Actions:<ul> <li>Force-Terminate EC2 Instances: Issues <code>TerminateInstances</code> commands to AWS for all identified instances.</li> <li>Purge DynamoDB State: Deletes or marks the corresponding DynamoDB records as <code>terminated</code>.</li> <li>Propagate Original Error: The error that triggered the cleanup is re-thrown for visibility.</li> </ul> </li> </ul> <p>Note</p> <p>The \"Successful Provisioning\" path is the desired outcome. The \"Unsuccessful Provisioning\" path handles expected capacity shortfalls. The \"Unhandled Error Cleanup\" is a guardrail against unforeseen issues.</p> <p></p>"},{"location":"architecture/provision/provision/","title":"Provision: Providing Compute \u26a1","text":"<p>Provision is a core operational mode of the controlplane designed to ensure GitHub Actions workflows always have suitable the EC2 instances required to execute CI jobs. Provision achieves these goals through two sub-components:</p> <ol> <li>Selection: Prioritizes reuse by selecting suitable idle runners from the resource pool.</li> <li>Creation: Provisioning new EC2 resources on-demand.</li> </ol>"},{"location":"architecture/provision/provision/#selection","title":"Selection","text":"<p>Selection is the reuse-first branch of Provision. Its sole mission is to satisfy a workflow\u2019s compute request without launching new EC2 capacity. It does this by rapidly scanning the Resource Pool, filtering messages that match the workflow\u2019s constraints, and atomically claiming (locking) any suitable idle runners.</p> <p>High\u2011level flow</p> <ol> <li>Pickup Manager dequeues messages from the SQS\u2011backed Resource Pool and filters by static attributes (<code>usageClass</code>, etc.).</li> <li>Invalid messages are generally requeued, valid messages are handed to Claim Workers (spawned in parallel, one per requested runner).</li> <li>Each Claim Worker performs an atomic idle-&gt;claimed transition in DynamoDB.  <ul> <li>On success: runs final health + registration checks and hands the instance to Post-Provision.  </li> <li>On failure: the worker asks the Pickup Manager for another candidate.</li> </ul> </li> <li>If workers report pool exhausted before compute is fulfilled, Creation sub\u2011component,is the fallback to launch fresh instances.</li> </ol> <p>Distilled Interaction Between Resource Pool, Pickup Manager and Claim Worker(s)</p> <pre><code>sequenceDiagram\n   participant ResourcePool as \"Resource Pool (SQS)\"\n   participant PickupManager\n   participant ClaimWorker as \"Claim Worker(s)\"\n   participant DynamoDB\n\n   Note over PickupManager, ClaimWorker: Pickup Manager and Claim Workers Initialized\n   ClaimWorker-&gt;&gt;PickupManager: Request for an instance\n   PickupManager-&gt;&gt;ResourcePool: Request Instance Message\n   ResourcePool--&gt;&gt;PickupManager: Pickup Instance Message\n   PickupManager-&gt;&gt;PickupManager: Filter messages (by usageClass, etc.)\n   PickupManager-&gt;&gt;ClaimWorker: Assign valid instance candidate\n   ClaimWorker-&gt;&gt;DynamoDB: Attempt atomic claim &lt;br&gt; (idle-&gt;claimed)\n   alt Claim Successful\n      DynamoDB--&gt;&gt;ClaimWorker: Claim successful \u2705\n      ClaimWorker-&gt;&gt;ClaimWorker: Run health &amp; registration checks \u2705\n      Note over ClaimWorker: Hand over claimed instance to Post Provision\n   else Claim Fails or Pool Exhausted\n      DynamoDB--&gt;&gt;ClaimWorker: Claim fails \u274c\n      ClaimWorker-&gt;&gt;PickupManager: Request new candidate \u267b\ufe0f\n      PickupManager-&gt;&gt;ResourcePool: Request for Instance Message\n      ResourcePool--&gt;&gt;PickupManager: Queue is empty\n      PickupManager--&gt;&gt;ClaimWorker: Return null \n      Note over ClaimWorker: With pool exhausted with no claimed compute, fallback to Creation\n   end</code></pre> <p>With that context, we'll cover the interfaces used by the Resource Pool, Pickup Manager and Claim Workers.</p> <p>See linked pages:</p> <ul> <li>Pickup Manager</li> <li>Claim Workers</li> </ul>"},{"location":"architecture/provision/provision/#creation-provisioning-new-instances","title":"Creation (Provisioning New Instances)","text":"<p>If the resource pool simply can\u2019t meet the workflow\u2019s needs - provision falls back to Creation. Here's a high\u2011level flow</p> <ol> <li> <p>Determine unmet capacity    Provision calculates how many additional runners are still required after Selection has finished, based on the workflow\u2019s <code>instance-count</code> and resource specs.</p> </li> <li> <p>Launch fleet request    A single <code>CreateFleet</code> call asks AWS for the exact number and mix of instance types that satisfy CPU, memory, and <code>usageClass</code> constraints.</p> </li> <li> <p>Seed the state store    As soon as AWS returns instance IDs, each one is recorded in DynamoDB with <code>state=created</code>, an initial <code>runId</code>, and a short <code>threshold</code> to set a reasonable lifetime.</p> </li> <li> <p>Fleet\u2011validation    Provision waits for two signals:  </p> <ul> <li>Successful Registration \u2013 the runner has completed user\u2011data bootstrap and registered with GitHub Actions (see Instance Initialization).  </li> <li>Heartbeat \u2013 the runner is emitting heartbeats.    All targets must report healthy within a timeout; otherwise creation is marked failed and cleanup begins (ie. all or nothing)</li> </ul> </li> </ol> <p>Distilled interaction with Fleet Creation/Validation and Instance via Indirect Signalling</p> <pre><code>sequenceDiagram\n   participant AWS\n   participant Provision\n   participant DynamoDB\n   participant Instance as \"Instance(s)\"\n\n   Note over Provision: Resource pool exhausted\n   Provision-&gt;&gt;Provision: Determine unmet capacity (required vs. claimed)\n   Provision-&gt;&gt;AWS: CreateFleet request\n   AWS--&gt;&gt;Provision: Return instance IDs\n   Provision-&gt;&gt;DynamoDB: Seed state store &lt;br&gt; (state=created, runId, threshold)\n   Provision-&gt;&gt;+DynamoDB: Wait for Signals &lt;br&gt; (Successful Registration, Heartbeat)\n\n   Note over Instance: Instance boots\n   Instance-&gt;&gt;Instance: Run user-data bootstrap\n   Instance--&gt;DynamoDB: Begin emitting heartbeats \u267b\ufe0f \n   Note over Instance: Register with Github\n   Instance-&gt;&gt;DynamoDB: Signal successful registration\n   Note over Instance: Await for CI Jobs \u267b\ufe0f\n\n   alt All signals received within timeout\n      DynamoDB-&gt;&gt;-Provision: Signals found within timeout\n      Provision-&gt;&gt;DynamoDB: Update state (created-&gt;running)\n      Note over Provision: Provisioning successful \u2705\n   else Timeout or failure\n      DynamoDB--&gt;&gt;Provision: Missing signals\n      Provision-&gt;&gt;AWS: Cleanup/terminate instances\n      Note over Provision: Provisioning failed \u274c\n   end</code></pre> <p>See linked pages for more detail</p> <ul> <li>Fleet Creation</li> <li>Fleet Validation</li> </ul>"},{"location":"architecture/provision/provision/#post-provisioning-actions","title":"Post-Provisioning Actions","text":"<p>The Post-Provision component is the final gate before the workflow can start running jobs (on success) - or before the control-plane rolls everything back (on failure).  </p> <p>See linked page for more detail:</p> <ul> <li>Post Provision</li> </ul> <p></p>"},{"location":"architecture/provision/creation/fleet-creation/","title":"Fleet Creation","text":""},{"location":"architecture/provision/creation/fleet-creation/#aws-api-used-createfleet-type-instant","title":"AWS API Used - <code>CreateFleet (type =instant)</code>","text":"<p>Provision issues a single <code>CreateFleet</code> call with Type=instant, which tells AWS to indicate to us if at this moment in time, there's enough in their capacity pools to fulfill our request or or fail fast. This keeps the controlplane responsive.</p> Simplified Create Fleet Call Input <p>Say 3 on-demand instances <pre><code>{\n  \"Type\": \"instant\",\n  \"TargetCapacitySpecification\": {\n    \"TotalTargetCapacity\": 3,\n    \"DefaultTargetCapacityType\": \"on-demand\",\n    \"OnDemandTargetCapacity\": 3\n  },\n  \"LaunchTemplateConfigs\": [ \u2026 ],\n}\n</code></pre></p> <p>If AWS cannot fulfil every requested instance (e.g., InsufficientInstaceCapacity), Provision aborts the fleet, cleans up any partial capacity, and surfaces an error back to the workflow. We also clean up the partial capacity by sending TerminateInstances just in case.</p>"},{"location":"architecture/provision/creation/fleet-creation/#attribute-based-instance-type-selection","title":"Attribute-Based Instance Type Selection","text":"<p>Instead of hard-coding instance types, the fleet uses attribute-based filters so AWS can pick any instance family that satisfies the request, dramatically improving hit rates in constrained regions.</p> Distilled example of the Launch Template overrides within Provision <p><pre><code>\"LaunchTemplateConfigs\": [\n  {\n    \"LaunchTemplateSpecification\": { \"LaunchTemplateId\": \"lt-abc123\", \"Version\": \"$Default\" },\n    \"Overrides\": [\n      {\n        \"InstanceRequirements\": {\n          \"VCpuCount\":   { \"Min\": 4, \"Max\": 4 },\n          \"MemoryMiB\":   { \"Min\": 4096 },\n          \"IncludedInstanceTypes\": [\"c*\", \"m*\"],\n        },\n        \"SubnetId\": \"subnet-aaa\u2026\",\n      }\n    ]\n  }\n]\n</code></pre> How this maps to the Provision interface:</p> Workflow Input Fleet Mapping <code>allowed-instance-types: \"c* m*\"</code> <code>IncludedInstanceTypes</code> mapped directly <code>resource-class: large</code> Populates <code>VCpuCount</code> &amp; <code>MemoryMiB</code> ranges"},{"location":"architecture/provision/creation/fleet-creation/#why-attribute-based-matters","title":"Why attribute-based matters","text":"<ul> <li>Maximises success probability: any size-compatible c* family (e.g., c6i, c7g) can be chosen.</li> <li>Reduces operator toil: no need to update docs every time AWS launches a new generation.</li> <li>Access to Multi-AZ Capacity Pools: Provision populates one override per subnet, so the fleet can pull capacity from whichever AZ still has it.</li> </ul> <p>Once AWS returns the instance IDs, each new runner is inserted into DynamoDB like so:</p> <pre><code>{\n  \"instanceId\": \"i-0abc12345\",\n  \"state\": \"created\",\n  \"runId\": \"run-7890\",\n  \"threshold\": \"2025-05-31T12:00:00Z\"\n}\n</code></pre> <p>From here, the Instance Initialization &amp; Fleet Validation logic (described in the next subsection) takes over, ensuring every newly created runner is healthy, registered, and transitioned to running.</p>"},{"location":"architecture/provision/creation/fleet-creation/#handling-insufficient-capacity","title":"Handling Insufficient Capacity","text":"<p>If any part of the request fails (including partial fulfilment):</p> <ol> <li>Provision logs the <code>CreateFleet</code> error.  </li> <li>Immediately issues a single <code>TerminateInstances</code> for every ID returned.  </li> <li>Surfaces a clear failed error to the workflow.</li> </ol> <p></p>"},{"location":"architecture/provision/creation/fleet-validation/","title":"Fleet Validation","text":"<p>After the control-plane successfully requests new EC2 instances from AWS, a critical validation process must confirm that every new instance has successfully bootstrapped, registered with GitHub, and is healthy before it can be used.</p> <p>This process is governed by a strict, all-or-nothing principle: if even a single instance in the newly created fleet fails its validation checks, the entire fleet is considered compromised and is terminated. This ensures that only fully operational and reliable sets of runners are introduced into the active pool.</p>"},{"location":"architecture/provision/creation/fleet-validation/#the-validation-handshake","title":"The Validation Handshake \ud83e\udd1d","text":"<p>Validation relies on a handshake between the control-plane and the new instances, using DynamoDB as the coordination point. This avoids direct communication and creates a robust, event-driven process.</p> <p>The sequence is as follows:</p> <ol> <li> <p>Control-Plane Initiates &amp; Assigns <code>runId</code>: For each new EC2 instance, the control-plane creates a record in DynamoDB. This initial record has its state set to <code>created</code> and, most importantly, includes the <code>runId</code> of the workflow that requested it.</p> <pre><code>// Initial record created by control-plane in DynamoDB\n{\n  \"instanceId\": \"i-abcdef123456\",\n  \"state\": \"created\",\n  \"runId\": \"workflow-run-789\", // Assigned by control-plane\n  \"threshold\": \"2025-06-01T10:05:00Z\"\n}\n</code></pre> </li> <li> <p>Instance Acts on <code>runId</code>: The bootstrap script on the new instance (detailed in Instance Initialization) polls its DynamoDB record. Once it detects the <code>runId</code>, it uses that ID to register itself as a GitHub Actions runner. This <code>runId</code> becomes the runner's label, ensuring it is targeted by the correct workflow jobs.</p> </li> <li> <p>Instance Signals Back: Upon successful registration with GitHub, the instance writes a registration signal (e.g., <code>UD_REG_OK</code>) back to its DynamoDB record. In parallel, a <code>heartbeat.sh</code> agent sends periodic pings to DynamoDB to signal its liveness.</p> </li> <li> <p>Control-Plane Observes: The control-plane's fleet validation routine polls DynamoDB, waiting for the expected registration and heartbeat signals from every instance in the fleet.</p> </li> </ol> <p>Sequence Diagram: Successful Fleet Validation</p> <pre><code>sequenceDiagram\n    participant CP as \"ControlPlane\"\n    participant DDB as \"DynamoDB\"\n    participant Instance1\n    participant Instance2\n\n    Note over CP, Instance2: Fleet Creation successfully creates Instance1 &amp; Instance2\n    Note over DDB, Instance2: Instance1 &amp; Instance2 periodically write heartbeat soon after startup \u267b\ufe0f \n    CP-&gt;&gt;DDB: Create record for Instance 1 &amp; Instance 2&lt;br&gt;(state: created, runId: 123)\n    CP-&gt;&gt;+DDB: Poll for signals from all instances\n\n    Instance1-&gt;&gt;+DDB: Poll for runId\n    DDB--&gt;&gt;-Instance1: Found runId: 123\n    Instance1-&gt;&gt;DDB: Write Registration Signal (UD_REG_OK) \ud83d\udc4c\n\n    Instance2-&gt;&gt;+DDB: Poll for runId\n    DDB--&gt;&gt;-Instance2: Found runId: 123\n    Instance2-&gt;&gt;DDB: Write Registration Signal (UD_REG_OK) \ud83d\udc4c\n\n    DDB--&gt;&gt;-CP: All signals and heartbeats are OK&lt;br&gt;(within timeout)\n\n    CP-&gt;&gt;DDB: Update Instance 1 &amp; Instance 2&lt;br&gt;(state: created -&gt; running)\n    Note right of CP: Fleet is validated as successful \u2705</code></pre>"},{"location":"architecture/provision/creation/fleet-validation/#validation-logic-outcomes","title":"Validation Logic &amp; Outcomes","text":"<p>The control-plane's validation process waits for two conditions to be met for the entire fleet before a validation timeout expires.</p> <ul> <li> <p>Success: If all instances pass both registration and heartbeat checks, the control-plane transitions their state in DynamoDB from <code>created</code> to <code>running</code>, and the fleet is ready for use.</p> <pre><code>// Record updated by control-plane after successful validation\n{\n  \"instanceId\": \"i-abcdef123456\",\n  \"state\": \"running\", // &lt;-- Updated\n  \"runId\": \"workflow-run-789\",\n  \"threshold\": \"2025-06-01T12:00:00Z\" // New threshold for running state\n}\n</code></pre> </li> <li> <p>Failure: If any instance fails a check or the validation timeout is reached, the control-plane terminates every instance in the fleet. The creation process is marked as failed, preventing a partially-healthy or unreliable fleet from being used.</p> </li> </ul> <p>Sequence Diagram: 1 out of 2 Instances Fail to Register</p> <pre><code>    sequenceDiagram\n        participant CP as \"ControlPlane\"\n        participant DDB as \"DynamoDB\"\n        participant Instance1\n        participant Instance2\n        participant AWS\n\n        Note over CP, AWS: Fleet Creation successfully creates Instance 1 &amp; Instance 2\n        Note over DDB, Instance2: Instance1 &amp; Instance2 periodically write heartbeat soon after startup \u267b\ufe0f \n        CP-&gt;&gt;DDB: Create record for Instance 1 &amp; 2 (state: created, runId: 123)\n\n        CP-&gt;&gt;+DDB: Poll for signals from all instances (Validation Timeout Starts)\n\n        Instance1-&gt;&gt;+DDB: Poll for runId\n        DDB--&gt;&gt;-Instance1: Found runId: 123\n        Instance1-&gt;&gt;DDB: Write Registration Signal (UD_REG_OK) \ud83d\udc4c\n\n        Note over Instance2: Fails to boot or register.&lt;br&gt;Never signals `UD_REG_OK`.\n\n        loop Until Validation Timeout\n            DDB--&gt;&gt;CP: Still waiting for signal from Instance 2...\n        end\n\n        DDB--&gt;&gt;-CP: Final response: Polling timed out.\n\n        Note right of CP: Instance 2 failed the check.&lt;br&gt;Terminating fleet.\n\n        CP-&gt;&gt;AWS: TerminateInstances(Instance1, Instance2)\n        AWS-&gt;&gt;Instance1: Terminate\n        AWS-&gt;&gt;Instance2: Terminate\n\n        Note right of CP: Fleet is validated as failed \u274c</code></pre> <p></p>"},{"location":"architecture/provision/selection/claim-workers/","title":"Claim Workers","text":"<p>Claim Workers are asynchronous tasks that run in parallel to secure idle runners for a workflow. Each worker executes a loop to pick up, claim, and verify an instance from the resource pool. The core responsibilities are:</p> <ul> <li>Atomic Claim: Perform a conditional update in DynamoDB to transition an instance's state from idle to claimed, guaranteeing exclusive ownership.</li> <li>Health Verification: Check the instance's heartbeat in DynamoDB to ensure it's responsive.</li> <li>Registration Check: Wait for a signal from the instance's agent confirming it's registered and ready for work.</li> </ul> <p>If an instance fails any of these checks, the worker discards it-terminating it if necessary-and attempts to claim a new one from the pool. This ensures that only fully validated runners are provisioned.</p> <p>Sequence Diagram: Successful Claim and Checks</p> <pre><code>sequenceDiagram\n    participant CW as \"Claim Worker\"\n    participant DDB as \"DynamoDB\"\n    participant Instance\n    participant GitHub\n\n    Note right of CW: Receives candidate_instance_id&lt;br&gt;from Pickup Manager\n    Instance--&gt;DDB: Periodically emit Heartbeat \u267b\ufe0f\n\n    CW-&gt;&gt;+DDB: Attempt Atomic Claim: Update instance_id&lt;br&gt;(idle-&gt;claimed, set run_id)\n    DDB--&gt;&gt;-CW: Claim Successful\n\n    Note over Instance, DDB: Instance polls DB, detects new run_id\n    CW-&gt;&gt;+DDB: Poll for Registration Signal&lt;br&gt;(instance_id, expected run_id, UD_REG_OK)\n\n    Instance-&gt;&gt;+GitHub: Register with run_id\n    GitHub--&gt;&gt;-Instance: Registration OK\n\n    Instance-&gt;&gt;DDB: Write Worker Signal (UD_REG_OK, run_id)\n\n    Instance--&gt;GitHub: Able to pickup CI Jobs \u267b\ufe0f \n\n    DDB--&gt;&gt;-CW: Signal OK\n    CW-&gt;&gt;+DDB: Poll for Heartbeat (instance_id)\n    DDB--&gt;&gt;-CW: Heartbeat OK (recent)\n\n    Note right of CW: Instance claimed, healthy, and registered \u2705\n    Note right of CW: Instance details handed to post-provision</code></pre>"},{"location":"architecture/provision/selection/claim-workers/#instance-claiming-process","title":"Instance Claiming Process","text":"<ol> <li>Receive Candidate Instance: A Claim Worker receives a candidate instance message from the Pickup Manager. The controlplane expects this message to describe an idle instance.</li> <li>Attempt Atomic State Update in DynamoDB: The worker attempts a single,     atomic conditional write operation in DynamoDB.<ul> <li>Condition: The operation only succeeds if the instance is currently in an <code>idle</code> state and has no <code>runId</code> associated with it.</li> <li>Mutation: If the condition is met, the instance's state is updated to <code>claimed</code>, a new <code>runId</code>, and a <code>threshold</code> (claimTimeout) is set.</li> <li>Significance of <code>runId</code> Assignment: This <code>runId</code> assignment is the primary trigger for the instance to register itself against Github. This id is used as its label in order to properly associate the workflow's CI jobs to the instance (see Instance Initialization).</li> </ul> </li> <li>Collision Handling: If the conditional write to DynamoDB fails (ie. another Claim Worker claimed the instance), the worker simply requests another candidate from the Pickup Manager to try again.</li> </ol> <p>Example record transition in DynamoDB:</p> <p>BEFORE (idle state):</p> <pre><code>{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"idle\",\n  \"runId\": \"\",\n  \"threshold\": \"2025-05-31T12:20:00Z\"\n}\n</code></pre> <p>AFTER (claimed state):</p> <pre><code>{\n  \"instanceId\": \"i-123456\",\n  \"state\": \"claimed\",\n  \"runId\": \"run-9999\",\n  \"threshold\": \"2025-05-31T12:30:00Z\"\n}\n</code></pre> <p>Claim Lifetime (<code>threshold</code>) versus Registration &amp; Health Checks</p> <p>The <code>threshold</code> set during the claim limits how long the instance can remain in the <code>claimed</code> state without successfully completing its registration and health checks. This ensures that if an instance gets stuck during this phase, the claim will eventually self-expire and will be terminated by the controlplane.</p>"},{"location":"architecture/provision/selection/claim-workers/#post-claim-checks-verifying-health-and-registration","title":"Post-Claim Checks: Verifying Health and Registration","text":"<p>Once an instance is successfully marked as <code>claimed</code> in DynamoDB, the Claim Worker polls for two key signals that the instance writes into DynamoDB. This indirect signaling mechanism means that we do not need to directly call AWS/GitHub APIs for these checks, mitigating potential API rate limit issues.</p> <p>Before the worker considers the claim fully successful and resolves its promise, it verifies:</p> Check Verification Method Pass Condition Timeout Heartbeat Checks the <code>HB</code> item for the instance in DynamoDB. The <code>updatedAt</code> timestamp of the heartbeat must be within the <code>HEARTBEAT_HEALTH_TIMEOUT</code>. ~15 s Registration Checks the <code>WS</code> (Worker Signal) item for the instance in DynamoDB. The signal value must be <code>UD_REG_OK</code>, and the <code>runId</code> in the signal must match the current workflow's <code>runId</code>. ~10 s <p>Heartbeat Record in DynamoDB (Example): (Primary Key: <code>TYPE#Heartbeat</code>, Sort Key: <code>ID#i-123</code>)</p> <pre><code>{\n  \"value\": \"PING\",\n  \"updatedAt\": \"2025-05-31T12:27:05Z\"\n}\n</code></pre> <p>This record is continuously updated by the <code>heartbeat.sh</code> agent running on the instance, as described in Instance Initialization.</p> <p>Registration Signal Record in DynamoDB (Example): (Primary Key: <code>TYPE#WS</code>, Sort Key: <code>ID#i-123</code>)</p> <pre><code>{\n  \"value\": {\n    \"signal\": \"UD_REG_OK\",\n    \"runId\": \"run-9999\"\n  }\n}\n</code></pre> <p>This signal is emitted by the instance itself after it successfully completes its GitHub registration process using the assigned <code>runId</code>. This entire registration sequence by the instance is detailed in Instance Initialization.</p> <p>Health Timeout and Signal Timeouts</p> <p>Currently, the specific timeout values for heartbeat validation and registration signal polling are internal to the control-plane and not externally tunable.</p>"},{"location":"architecture/provision/selection/claim-workers/#process-outcomes","title":"Process Outcomes","text":"<p>The claiming process concludes in one of two ways:</p> <ul> <li> <p>Success: When both heartbeat and registration checks pass, the Claim Worker returns the verified instance details to the Provision orchestrator. The orchestrator then transitions the instance's state to <code>running</code>, making it ready for workflow jobs.</p> </li> <li> <p>Failure: If either check fails, the worker deems the instance non-viable. It expires the instance's DynamoDB record (marking it for cleanup) and may directly terminate the EC2 instance. The worker then signals failure to the orchestrator, which can either instruct a retry with a new candidate or, if the pool is exhausted, trigger the provisioning of new capacity (ie. Creation).</p> </li> </ul> <p>Sequence Diagram: Successful Claim but Failed Checks</p> <pre><code>sequenceDiagram\n    participant CW as \"Claim Worker\"\n    participant DDB as \"DynamoDB\"\n\n    participant Instance\n    participant AWS\n\n    Note right of CW: Receives candidate_instance_id&lt;br&gt;from Pickup Manager\n\n    CW-&gt;&gt;+DDB: Attempt Atomic Claim (idle -&gt; claimed, set run_id)\n    DDB--&gt;&gt;-CW: Claim Successful\n\n    Note over Instance, DDB: Instance detects new run_id&lt;br&gt;but fails to register with GitHub&lt;br&gt;or crashes.\n\n    CW-&gt;&gt;+DDB: Poll for Registration Signal (UD_REG_OK)\n\n    loop For ~10 seconds\n        DDB--&gt;&gt;CW: Signal not found...\n    end\n\n    DDB--&gt;&gt;-CW: Final response: Polling timed out \u274c\n\n    Note right of CW: Instance is non-viable. Terminating.\n\n    CW-&gt;&gt;AWS: Terminate Instance (instance_id)\n    AWS-&gt;&gt;Instance: Terminate Instance\n    Note over Instance: Shutdown\n\n    CW-&gt;&gt;DDB: Mark instance as expired in DB\n\n    Note right of CW: Retry process with &lt;br&gt;new candidate from Pickup Manager</code></pre> <p></p>"},{"location":"architecture/provision/selection/pickup-manager/","title":"Pickup Manager","text":"<p>The Pickup Manager (PM) is an in-process component instantiated per workflow selection attempt. It is responsible for retrieving and filtering idle instance messages from the SQS-based Resource Pool to find suitable warm runners for the requestors (Claim Workers) to fulfill the current workflow's requirements.</p> <p>The Pickup Manager's core responsibilities are:</p> <ol> <li>Dequeueing Candidate Messages: Retrieving messages from the relevant SQS resource pool queue.</li> <li>Filtering: Evaluating the dequeued message (whose format is detailed in the Resource Pool documentation) against the requesting workflow\u2019s specific compute requirements (e.g., instance type, usage class, CPU, memory).</li> <li>Dispatching or Re-queuing:<ul> <li>\u2705 If a message represents a suitable instance, it's passed to a Claim Worker for an attempt to claim the instance.</li> <li>\u267b\ufe0f If unsuitable for the current request, the message is returned to the SQS queue for other workflows.</li> <li>\u274c If the message represents an invalid or malformed entry, it may be discarded.</li> </ul> </li> </ol> <p>Sequence Diagram: Valid Message</p> <pre><code>sequenceDiagram\n    participant ClaimWorker\n    participant PM as \"Pickup Manager\"\n    participant SQS_Pool as \"Resource Pool (SQS)\"\n\n    Note over PM: Intitialized with workflow's compute requirements &lt;br&gt;(resource class, usage class, etc.)\n    Note over PM, SQS_Pool: Reference a specific queue from pool. &lt;br&gt; Dedicated queue per resource class\n    ClaimWorker-&gt;&gt;+PM: Request Instance\n\n    %% Start of the conceptual \"loop\" for a single successful attempt\n    PM-&gt;&gt;+SQS_Pool: Dequeue Message\n    SQS_Pool--&gt;&gt;-PM: instance_message_data (with instance_id)\n\n    PM-&gt;&gt;PM: Check Pickup Frequency (for instance_id)\n    %% Happy Path: Frequency is OK\n\n    PM-&gt;&gt;PM: Compare instance_message_data.payload vs compute requirements\n    %% Happy Path: Message is OK (Suitable)\n\n    PM--&gt;&gt;-ClaimWorker: Return InstanceMessage &lt;br&gt; (instance_message_data.payload)\n    Note over ClaimWorker: Begin Claiming routine &lt;br&gt; Will re-request from pool if needed</code></pre>"},{"location":"architecture/provision/selection/pickup-manager/#1-interfacing-with-sqs-the-pickup-lifecycle","title":"1. Interfacing with SQS: The Pickup Lifecycle","text":"<p>The Pickup Manager interacts with SQS in a tight loop designed for low latency and efficient message handling. This process involves several conceptual SQS operations:</p> Action by Pickup Manager Conceptual SQS Operation Purpose &amp; Notes 1 Request a message from the resource class-specific SQS queue. <code>ReceiveMessage</code> A short poll used for discovering available idle runners. 2 Delete the message from SQS. <code>DeleteMessage</code> As soon as received, delete the message from the queue to minimize contention by multiple concurrent Provision processes or workers 3 Filter the message content in-memory. N/A (Local operation) The contents of message checked against the workflow\u2019s provision inputs (e.g., <code>allowedInstanceTypes</code>, <code>usageClass</code>, etc.) 4a  Pass-On (Match Found) N/A (Dispatch to Claim Worker) If check is OK - PM hands to requesting Claim Worker 4b  Re-queue (Filter Mismatch) <code>SendMessage</code> If mismatching current workflow's compute constraints - message is requeued with a short delay to minimize contention 4c  Discard (Invalid/Malformed Message) N/A (Already deleted) Something is wrong with the message (ie. parsing, unexpected attributes), it is discarded"},{"location":"architecture/provision/selection/pickup-manager/#2-message-handling-and-classification-logic","title":"2. Message Handling and Classification Logic","text":"<p>The Pickup Manager evaluates each dequeued message to determine its fate based on the workflow's requirements:</p> <ul> <li> <p>\u2705 OK (Pass-On): The message's attributes (instance type, usage class, CPU/memory) are a suitable match.</p> <ul> <li>Action: The message is passed to a Claim Worker to attempt claiming the instance.</li> </ul> </li> <li> <p>\u267b\ufe0f Re-queue: The instance is valid but doesn't meet the current workflow's specific filters (e.g., a different instance type is needed).</p> <ul> <li>Action: The message is sent back to its SQS queue, making it available for other, potentially more suitable, workflow requests.</li> </ul> </li> <li> <p>\u274c Discard: The message is malformed, describes an instance with specs that don't match its resource class, or is otherwise invalid.</p> <ul> <li>Action: The message is logged and effectively dropped, as it was already deleted from the SQS queue upon receipt.</li> </ul> </li> </ul>"},{"location":"architecture/provision/selection/pickup-manager/#3-pool-exhaustion-detection","title":"3. Pool Exhaustion Detection","text":"<p>The Pickup Manager can determine that a resource pool is \"exhausted\" for the current workflow's request through two main heuristics:</p> <ul> <li>Globally Exhausted (Queue Empty):<ul> <li>If an attempt to receive a message from the SQS queue indicates that the queue for the target <code>resourceClass</code> is currently empty.</li> <li>Outcome: The Pickup Manager signals to the Provision orchestrator that no idle instances are available in this pool at this moment.</li> </ul> </li> </ul> <p>Sequence Diagram: Global Exhaustion</p> <pre><code>sequenceDiagram\n    participant ClaimWorker\n    participant PM as \"Pickup Manager\"\n    participant SQS_Pool as \"SQS Resource Pool\"\n\n    ClaimWorker-&gt;&gt;+PM: Attempt Pickup (workflow_requirements)\n\n    PM-&gt;&gt;+SQS_Pool: Request Message\n    SQS_Pool--&gt;&gt;-PM: No message \ud83e\udd37\n\n    PM--&gt;&gt;-ClaimWorker: Return Null (Locally Exhausted for this workflow)</code></pre> <ul> <li>Locally Exhausted (Repetitive Unsuitable Messages):<ul> <li>To prevent an infinite loop due to the Pickup Manager requeueing behavior with unsuitable messages, it maintains an internal frequency count for each unique <code>instanceId</code> it encounters during its current operational cycle.</li> <li>If the same <code>instanceId</code> is dequeued more than a defined tolerance threshold (ie., 5 times), the Pickup Manager considers the pool exhausted for the workflow.</li> <li>Outcome: The message that triggered this tolerance is still re-queued (so it remains available for other, potentially different, workflow requests). However, the Pickup Manager signals <code>null</code> any interfacing claim workers - effectively suspending pickups for this workflow.</li> </ul> </li> </ul> <p>Sequence Diagram: Local Exhaustion</p> <pre><code>sequenceDiagram\n    participant ClaimWorker\n    participant PM as \"Pickup Manager\"\n    participant SQS_Pool as \"Resource Pool (SQS)\"\n\n    ClaimWorker-&gt;&gt;+PM: Request Instance\n\n    Note over PM: Initialize frequency counter for instances\n\n    rect \n        Note right of PM: Loop: First instance pickup attempt\n        PM--&gt;SQS_Pool: Dequeue instance_A Message\n\n        Note over PM: Register instance_A (freq=1)&lt;br&gt;But unsuitable\n        PM--&gt;SQS_Pool: Re-queue instance_A\n\n    end\n\n    Note over PM: Additional cycles (truncated)\n\n    rect\n        Note right of PM: Loop: Later pickup attempts &lt;br&gt; imagine only instance_A is in Pool\n        PM--&gt;SQS_Pool: Dequeue instance_A Message (again)\n\n        Note over PM: Register instance_A (freq=n)&lt;br&gt;Still unsuitable\n        Note over PM: Frequency exceeds tolerance\n        PM--&gt;SQS_Pool: Re-queue instance_A\n    end\n\n    PM--&gt;&gt;-ClaimWorker: Return Null (Locally Exhausted)\n    Note over ClaimWorker: Claim worker informs &lt;br&gt;controlplane of exhaustion</code></pre> <p></p>"},{"location":"examples/advanced-scenarios/","title":"Advanced Workflow Scenarios","text":""},{"location":"examples/advanced-scenarios/#1-cost-optimization-on-demand-for-releases-spot-for-ci","title":"1. Cost Optimization: On-Demand for Releases, Spot for CI","text":"<p>Scenario - one workflow uses <code>spot</code> instances, another <code>on-demand</code> (good for critical workflows - ie. release). Each workflow will provision runners with its specified <code>usage-class</code>.</p> Sample Refresh Configuration (<code>.github/workflows/refresh.yml</code>) <p>Your <code>refresh.yml</code> remains standard. It manages the overall pool and AWS resources. <pre><code># .github/workflows/refresh.yml\nname: Refresh Runner Pool\n# ... (standard refresh configuration as in previous examples) ...\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"*/15 * * * *\"\n\njobs:\n  refresh_job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1 # Or your desired region\n      - name: Refresh EC2 Runner Pool\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          github-token: ${{ secrets.GH_PAT }}\n          ami: ami-xxxxxxxxxxxxxxxxx # Your chosen AMI ID\n          iam-instance-profile: YourInstanceProfileName\n          security-group-ids: sg-xxxxxxxxxxxxxxxxx\n          subnet-ids: subnet-xxxxxxxxxxxxxxxxx subnet-yyyyyyyyyyyyyyyyy\n</code></pre></p> CI Workflow with Spot Instances (<code>.github/workflows/main-ci.yml</code>) <p>This workflow provisions cost-effective <code>spot</code> instances for routine CI tasks. <pre><code># .github/workflows/main-ci.yml\nname: Main CI (Spot Instances)\n\non:\n  pull_request:\n  push:\n    branches: [main, develop]\n\njobs:\n  provision_spot_runners:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        # ... (AWS credentials configuration) ...\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Provision Spot Runners for CI\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          instance-count: 3\n          usage-class: spot # &lt;----\n\n  build_and_test:\n    needs: provision_spot_runners\n    runs-on: ${{ github.run_id }} # Uses runners provisioned for this workflow run\n    strategy:\n      matrix:\n        node-version: [18, 20]\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Use Node.js ${{ matrix.node-version }}\n        # ... (setup node) ...\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby-version }}\n      - name: Build and Test\n        run: |\n          echo \"Running CI on spot instances...\"\n          # Your build and test commands\n\n  release_spot_runners:\n    needs: [provision_spot_runners, build_and_test]\n    runs-on: ubuntu-latest\n    if: ${{ always() }}\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Release Spot Runners\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: release\n</code></pre></p> Release Workflow with On-Demand Instances (<code>.github/workflows/release.yml</code>) <p>This workflow provisions reliable <code>on-demand</code> instances for critical release tasks. <pre><code># .github/workflows/release.yml\nname: Release Workflow (On-Demand)\n\non:\n  workflow_dispatch: # Manual trigger for releases\n  push:\n    tags:\n      - 'v*' # Trigger on version tags\n\njobs:\n  provision_ondemand_runners:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        # ... (AWS credentials configuration) ...\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Provision On-Demand Runners for Release\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          instance-count: 1\n          usage-class: on-demand\n          allowed-instance-types: \"m5.large\" # Specific, reliable type\n          max-runtime-min: 120 # Potentially longer timeout for deployments\n\n  deploy_production:\n    needs: provision_ondemand_runners\n    runs-on: ${{ github.run_id }} # Uses on-demand runner from this workflow's pool\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Deploy to Production\n        run: echo \"Deploying to production using an on-demand runner...\"\n        # Your deployment script\n\n  release_ondemand_runners:\n    needs: [provision_ondemand_runners, deploy_production]\n    runs-on: ubuntu-latest\n    if: ${{ always() }}\n    steps:\n      - name: Configure AWS Credentials\n        # ... (AWS credentials configuration) ...\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Release On-Demand Runners\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: release\n</code></pre></p>"},{"location":"examples/advanced-scenarios/#2-specialized-environments-custom-cpu-resource-classes","title":"2. Specialized Environments: Custom CPU Resource Classes","text":"<p>To use runners with different CPU/memory profiles for various tasks (e.g., standard builds vs. CPU-intensive computations), you can define custom resource classes and provision them in separate workflow files.</p> Sample Refresh Configuration with Custom Resource Classes (<code>.github/workflows/refresh.yml</code>) <p>Define <code>resource-class-config</code> in your <code>refresh.yml</code> to specify your custom CPU and memory configurations. <pre><code># .github/workflows/refresh.yml\nname: Refresh Runner Pool with Custom CPU Classes\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"*/15 * * * *\"\n\njobs:\n  refresh_job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Refresh EC2 Runner Pool\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          github-token: ${{ secrets.GH_PAT }}\n          ami: ami-general-purpose-xxxxxxxx # A general-purpose AMI suitable for all tasks\n          iam-instance-profile: YourInstanceProfileName\n          security-group-ids: sg-xxxxxxxxxxxxxxxxx\n          subnet-ids: subnet-xxxxxxxxxxxxxxxxx\n          resource-class-config: |\n            {\n              \"standard-compute\": { \"cpu\": 2, \"mmem\": 4096 },\n              \"high-cpu-compute\": { \"cpu\": 8, \"mmem\": 16384 }\n            }\n          # Broad allowed-instance-types; specific workflows can refine this\n          allowed-instance-types: \"c* m* r*\"\n</code></pre></p> Standard Compute Workflow (<code>.github/workflows/ci-standard.yml</code>) <p>This workflow provisions <code>standard-compute</code> runners for general CI tasks. <pre><code># .github/workflows/ci-standard.yml\nname: Standard Compute CI\n\non:\n  pull_request:\n\njobs:\n  provision_standard_runners:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Provision Standard Compute Runners\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          instance-count: 2\n          resource-class: \"standard-compute\"\n          # Optional: Refine instance types if needed, e.g., general purpose\n          allowed-instance-types: \"m6* m5*\"\n\n  run_standard_tasks:\n    needs: provision_standard_runners\n    runs-on: ${{ github.run_id }}\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Execute standard build and test\n        run: echo \"Running standard CI tasks on 'standard-compute' runners...\"\n\n  release_standard_runners:\n    needs: [provision_standard_runners, run_standard_tasks]\n    runs-on: ubuntu-latest\n    if: ${{ always() }}\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Release Standard Compute Runners\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: release\n</code></pre></p> High CPU Compute Workflow (<code>.github/workflows/ci-high-cpu.yml</code>) <p>This workflow provisions <code>high-cpu-compute</code> runners specifically targeting CPU-optimized instance families for intensive tasks. <pre><code># .github/workflows/ci-high-cpu.yml\nname: High CPU Compute Workflow\n\non:\n  workflow_dispatch: # e.g., for nightly computationally intensive jobs\n\njobs:\n  provision_high_cpu_runners:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Provision High CPU Runners\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          instance-count: 1\n          resource-class: \"high-cpu-compute\"\n          # Target CPU-optimized instance families\n          allowed-instance-types: \"c6a.2xlarge c7g.2xlarge c5.2xlarge\"\n\n  run_intensive_computation:\n    needs: provision_high_cpu_runners\n    runs-on: ${{ github.run_id }}\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n      - name: Execute CPU-intensive task\n        run: |\n          echo \"Starting CPU-intensive computation on 'high-cpu-compute' runner...\"\n          # Your computationally demanding script or program\n\n  release_high_cpu_runners:\n    needs: [provision_high_cpu_runners, run_intensive_computation]\n    runs-on: ubuntu-latest\n    if: ${{ always() }}\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Release High CPU Runners\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: release\n</code></pre></p>"},{"location":"examples/basic-workflow/","title":"Basic Workflow Examples","text":""},{"location":"examples/basic-workflow/#using-a-matrix-strategy-example-for-ruby-versions","title":"Using a Matrix Strategy (Example: for Ruby Versions)","text":"<p>Prerequisites: Ensure you have completed the setup described in the Quickstart</p> Refresh Configuration <pre><code># .github/workflows/refresh.yml\nname: Refresh Runner Pool\n\non:\n  workflow_dispatch: # Allows manual triggering\n  schedule:\n    - cron: \"*/15 * * * *\" # Runs every 15 minutes\n\njobs:\n  refresh_job:\n    runs-on: ubuntu-latest # This job runs on a GitHub-hosted runner\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1 # Or your desired region\n      - name: Refresh EC2 Runner Pool\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          github-token: ${{ secrets.GH_PAT }}\n          ami: ami-xxxxxxxxxxxxxxxxx # Your chosen AMI ID\n          iam-instance-profile: YourInstanceProfileName # Your EC2 instance profile\n          security-group-ids: sg-xxxxxxxxxxxxxxxxx # Your security group\n          subnet-ids: subnet-xxxxxxxxxxxxxxxxx subnet-yyyyyyyyyyyyyyyyy # Your subnets\n</code></pre> Basic Ruby Matrix Workflow (<code>ci-ruby-matrix.yml</code>) <pre><code># .github/workflows/ci-ruby-matrix.yml\nname: CI Ruby Matrix Test\n\non:\n  pull_request:\n  push:\n    branches: [main]\n\njobs:\n  provision_runners:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1 # Or your desired region\n      - name: Provision Runners for Matrix\n        id: provision_step\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          # Adjust 'instance-count' based on your matrix size.\n          instance-count: 3\n\n\n  test_ruby_versions:\n    needs: provision_runners\n    runs-on: ${{ github.run_id }} # Uses runners provisioned for this workflow run\n    strategy:\n      matrix:\n        ruby-version: ['2.7', '3.0', '3.1'] # Define the Ruby versions to test against\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4 \n\n      - name: Set up Ruby ${{ matrix.ruby-version }}\n        uses: ruby/setup-ruby@v1\n        with:\n          ruby-version: ${{ matrix.ruby-version }}\n\n      - name: Install dependencies \n        run: bundle install\n\n      - name: Run RSpec tests (or your test command)\n        run: bundle exec rspec\n\n  release_runners:\n    needs: [provision_runners, test_ruby_versions]\n    runs-on: ubuntu-latest \n    if: ${{ always() }} # Important: Always release runners\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Release EC2 Runners\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: release\n</code></pre>"},{"location":"getting-started/advanced-configuration/","title":"Advanced Configuration","text":""},{"location":"getting-started/advanced-configuration/#1-introduction","title":"1. Introduction","text":"<p>This document explores the additional optional inputs given opened up to the controlplane. See the Prerequisites and Quickstart for the initial setup guides.</p> <p>Note</p> <p>Hopefully this page can stand by itself, but feel free to read through the Architecture to see this from a wider context.</p>"},{"location":"getting-started/advanced-configuration/#2-fine-tuning-runner-lifecycle-and-resource-management","title":"2. Fine-Tuning Runner Lifecycle and Resource Management","text":""},{"location":"getting-started/advanced-configuration/#understanding-runner-timeouts","title":"Understanding Runner Timeouts","text":"<p>Several inputs in the <code>refresh</code> mode control how long runners live and how often their configurations. Finding the right balance is crucial:</p> <p>Strategy </p> <p>Start with the defaults. If you notice frequent cold starts and your budget allows, consider increasing <code>idle-time-sec</code>. If cost is a primary concern, shorten <code>idle-time-sec</code> but monitor workflow times. Ensure <code>max-runtime-min</code> accommodates your longest jobs.</p>"},{"location":"getting-started/advanced-configuration/#idle-time-sec-lifetime-in-resource-pool","title":"<code>idle-time-sec</code>: Lifetime in Resource Pool","text":"<p>Defined at the <code>refresh</code> level with a default of 300s/5m. This defines how long the instance lives in the resource pool for following workflows to pickup! If in the pool for too long, the instance will be terminated by <code>refresh</code> or will undergo self-termination.</p> <pre><code>      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          # idle-time-sec: 300 # &lt;---- (Default: 300 seconds)\n</code></pre> <code>idle-time-sec</code> - Shorter or Longer  <ul> <li>Shorter: Reduces costs by terminating idle instances sooner. However, it might lead to more \"cold starts\" if subsequent jobs arrive just after an instance terminated, increasing wait times.</li> <li>Longer: Increases runner availability and reduces cold starts, but can lead to higher costs due to instances sitting idle for longer.</li> <li>Align this with your typical job arrival patterns. If jobs are infrequent, a shorter idle time might be better. If jobs are frequent, a longer idle time can improve CI/CD pipeline speed.</li> </ul>"},{"location":"getting-started/advanced-configuration/#max-runtime-min-expected-max-job-duration","title":"<code>max-runtime-min</code>: Expected Max Job Duration","text":"<p>Defined at the <code>refresh</code> level (default: 30 minutes) and overridable at the <code>provision</code> level, this parameter sets the maximum active duration for an instance between its assignment to a workflow and its release back to the pool.</p> <p>It acts as a critical safeguard, enabling the control plane to safely terminate instances. This prevents them from being stranded due to misconfigured <code>release</code> jobs or other unforeseen issues that would otherwise leave resources unreleased.</p> <pre><code>      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          # max-runtime-min: 30 # &lt;---- (Default: 30m)\n</code></pre> <p>Overriding at the <code>provision</code> level. The operator can set some expectations for the controlplane to say how long a specific workflow can take from <code>provision</code> to <code>release</code> and control this timeouts at a workflow level instead of at the repo level.</p> <pre><code>      - name: Provision Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          # max-runtime-min: 30 # &lt;---- (overrides refresh input if provided)\n</code></pre> <code>max-runtime-min</code> - Shorter or Longer  <ul> <li>Recommendation Be conservative. Whatever your longest running job is at a workflow, add 10-15 minutes to it to avoid pre-mature termination.</li> <li>This can be set at either the <code>refresh</code> or <code>provision</code> level to set per-workflow expectations.</li> </ul>"},{"location":"getting-started/advanced-configuration/#instance-purchasing-options-provision-mode","title":"Instance Purchasing Options (<code>provision</code> mode)","text":"<p>The <code>provision</code> mode offers ways to control the type and cost of EC2 instances provisioned for specific workflows.</p>"},{"location":"getting-started/advanced-configuration/#usage-class-on-demand-vs-spot","title":"<code>usage-class</code>: On-Demand vs Spot","text":"<p>You can specify whether to provision <code>on-demand</code> or <code>spot</code> instances at the workflow level using the <code>usage-class</code> input in <code>provision</code> mode. The default is <code>on-demand</code>. It's generally recommended to use <code>on-demand</code> instances for critical workflows and <code>spot</code> instances for non-critical CI tasks.</p> <pre><code>      - name: Provision Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          # usage-class: on-demand # &lt;---- (Default: on-demand)\n</code></pre> <p> The Right Match: On-Demand vs. Spot </p> <p>The controlplane is able to discriminate between instances which have <code>on-demand</code> or <code>spot</code> lifecycles. If none available, the contolplane creates specific instances faithful to the <code>usage-class</code> constraint.</p>"},{"location":"getting-started/advanced-configuration/#allowed-instance-types-instance-types-by-wildcards","title":"<code>allowed-instance-types</code>: Instance types by wildcards","text":"<p>This input accepts a space-separated list of EC2 instance types (e.g., <code>m5.large c6i* r*</code>) or family wildcards (e.g., <code>c*</code>, <code>m*</code>, <code>r*</code>) that the workflow should use. Instances matching these patterns will be selected from the existing resource pool or provisioned if new ones are needed. This capability aligns with the AWS AllowedInstanceTypes specification.</p> <p>The default is <code>c* m* r*</code>. This setting allows the control plane to choose from a wide array of instance types.</p> <pre><code>      - name: Provision Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          # allowed-instance-types: \"c* m* r*\" # &lt;---- (Default: \"c* m* r*\")\n</code></pre> <p>Be generous with <code>usage-class: spot</code> </p> <p>Telling the controlplane to use spot instances for the workflow? Cast your net wide! Defer to the generous <code>c* m* r*</code> defaults to ensure that AWS always has capacities for your instances when your workflow asks for these instances.</p> <p>The Right Match: <code>allowed-instance-types</code></p> <p>Similar to <code>usage-class</code>, the controlplane is able to discriminate instances from the shared resource pool given the patterns and instance types specified in <code>allowed-instance-types</code>.</p>"},{"location":"getting-started/advanced-configuration/#3-some-ami-and-pre-runner-script-strategies","title":"3. Some AMI and <code>pre-runner-script</code> strategies","text":"<p>Bake as much in to the AMI image as you can to ensure timely startup times. As per Quickstart, feel free to use Runs-On's machine images to not worry about any of this. However if you do decide to roll your own, there are a couple of hard requirements to ensure proper functionality:</p> <ul> <li> <code>git</code> &amp; <code>docker</code>: Checking out code and running containers</li> <li> <code>aws cli</code>: Controlplane communication via dynamodb</li> <li> <code>libicu</code>: Configuring instances against github actions as a self-hosted runner</li> </ul>"},{"location":"getting-started/advanced-configuration/#pre-runner-script","title":"<code>pre-runner-script</code>","text":"<p>Here are some recommended scripts when using various bare AMI images.</p> <p>Refining recommended scripts</p> <p>If any of these scripts do not work to initialize the runners, feel free to raise a pull request! It would be much appreciated ~ \ud83d\ude4f</p> Amazon Linux 2023 <pre><code>      - name: Provision Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          ami: ami-123 # &lt;--- AL2023 image\n          pre-runner-script: |\n            #!/bin/bash\n            sudo yum update -y &amp;&amp; \\\n            sudo yum install docker -y &amp;&amp; \\\n            sudo yum install git -y &amp;&amp; \\\n            sudo yum install libicu -y &amp;&amp; \\\n            sudo systemctl enable docker\n</code></pre> Ubuntu 24 <pre><code>      - name: Provision Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          ami: ami-123 # &lt;--- Ubuntu 24\n          pre-runner-script: |\n            #!/bin/bash\n            sudo apt update &amp;&amp; sudo apt upgrade -y\n            sudo apt install -y docker.io git libicu-dev unzip curl\n\n            # AWS CLI v2 --&gt; x86_64\n            curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n            unzip awscliv2.zip\n            sudo ./aws/install\n            rm -rf awscliv2.zip aws\n\n            sudo systemctl enable docker\n            sudo systemctl start docker\n</code></pre>"},{"location":"getting-started/advanced-configuration/#4-resource-classes-for-varied-workloads","title":"4. Resource Classes for Varied Workloads","text":"<p>The operator can control the size of the instances at the level of the workflow (ie. <code>provision</code>) - with sizes defined at the level of the repo (ie. <code>refresh</code>). The former is specifies the resource class of the instance for the workflow (via <code>resource-class</code>), and the latter specifies the valid resource classes (via <code>resource-class-config</code>).</p>"},{"location":"getting-started/advanced-configuration/#pre-defined-resource-classes-usage","title":"Pre-Defined Resource Classes &amp; Usage","text":"<p>The controlplane provides a comprehensive set of resource classes that can be used at a workflow level. At the time of this writing. the defaults are the following:</p> Resource Class CPU (cores) Minimum Memory (MB) large 2 4,096 xlarge 4 8,192 2xlarge 8 16,384 4xlarge 16 32,768 8xlarge 32 65,536 12xlarge 48 98,304 16xlarge 64 131,072 <p>And to use these pre-defined resource classes, simply specify the <code>resource-class</code> attribute at the <code>provision</code> level</p> <pre><code>      - name: Provision Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          resource-class: \"xlarge\" # &lt;---- (Default: \"large\")\n</code></pre> <p>Greater control with <code>allowed-instance-types</code></p> <p>Say that for a specific workflow, you have specified the following: <pre><code>      - name: Provision Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          resource-class: \"xlarge\"\n          allowed-instance-types: \"c6* m5*\"\n</code></pre> Then the controlplane will only pick up resources tagged with <code>xlarge</code> given that their instance types match with the <code>c6</code> and <code>m5</code> families. If none available from the pool, then only instances which fulfill these requirements will be provisioned.</p> <p>The Right Match: <code>resource-class</code></p> <p>Similar to <code>usage-class</code> and <code>allowed-instance-types</code> - the controlplane is able to discriminate instances from the shared resource pool given the specified <code>resource-class</code>.</p> Keeping consistent with AWS \ud83d\udcda <ul> <li>Naming Convention: Resource class names align with AWS EC2 instance type naming (e.g., <code>large</code> = 2 CPU cores, <code>xlarge</code> = 4 CPU cores, etc.)</li> <li>Memory Allocation: Memory values represent minimum requirements and are set at approximately 2GB per CPU core. This ensures compatibility across the mainstream instance families:<ul> <li>Compute-optimized instances (<code>c</code> family): ~2GB per core</li> <li>General-purpose instances (<code>m</code> family): ~4GB per core</li> <li>Memory-optimized instances (<code>r</code> family): ~8GB per core</li> </ul> </li> </ul>"},{"location":"getting-started/advanced-configuration/#custom-resource-classes","title":"Custom Resource Classes","text":"<p>If you have custom requiremetns - here's an example. Note that this this overrides the pre-defined resource classes.</p> <pre><code># in refresh.yml\n      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          resource-class-config: '{ \"custom-large\": { \"cpu\": 4, \"mmem\": 8000 }, \"custom-extra-large\": { \"cpu\": 8, \"mmem\": 16000 } }'\n</code></pre> <pre><code># in ci.yml\n      - name: Provision Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          resource-class: \"custom-large\"\n</code></pre>"},{"location":"getting-started/advanced-configuration/#5-permission-network-hardening","title":"5. Permission &amp; Network Hardening","text":"<p>Beyond the basic IAM policies in Prerequisites, feel free to further harden IAM Policies and the Networking around the provided subnets.</p>"},{"location":"getting-started/advanced-configuration/#iam-least-privilege","title":"IAM Least Privilege","text":""},{"location":"getting-started/advanced-configuration/#iam-entity-for-the-controlplane","title":"IAM Entity for the ControlPlane","text":"<ul> <li>When configuring credentials with <code>aws-actions/configure-aws-credentials</code>, it is recommended to use OIDC to prevent the usage of permanent credentials within CI.</li> <li> <p><code>iam:PassRole</code> - Once you know the exact role the controlplane is passing on to the instances, I highly recommend adding its ARN to the</p> <pre><code>\"Effect\": \"Allow\",\n\"Action\": [\"iam:PassRole\"],\n\"Resource\": // &lt;--- arn of the role\n</code></pre> </li> <li> <p>SQS &amp; DynamoDB - These resources are created with a repo-specific prefix which we can use to harden the IAM policy further <code>fleet-actions-ec2-runner-pool-manager</code></p> <pre><code>// DDB\n\"Resource\": \"arn:aws:dynamodb:*:*:table/{repo-owner}-{repo-name}-*\"\n// SQS\n\"Resource\": \"arn:aws:sqs:*:*:{repo-owner}-{repo-name}-*\"\n</code></pre> </li> </ul>"},{"location":"getting-started/advanced-configuration/#iam-entity-for-the-ec2-instance-profile","title":"IAM Entity for the EC2 Instance Profile","text":"<p>At a minimum, the role given to the instance needs to be able to self-terminate and read-write from a ddb table. As such, we can explore two avenues of hardening the role handed to the ec2 instance beyond the minimum defined in the Prerequisites:</p> <ul> <li>Self-Termination: Can use Self referencial ARNs to ensure that <code>aws ec2 terminate-instances</code> can only be really delivered to the instance that calls it.</li> <li>Read &amp; Write DynamoDB Table: As above, restrict the resource to an arn that follows the repo-owner and repo-name <code>\"arn:aws:dynamodb:*:*:table/{repo-owner}-{repo-name}-*\"</code></li> </ul> <p>Interacting with other AWS Services \u2601\ufe0f</p> <p>If your self-hosted runner needs to communicate with other AWS services (ie. <code>s3</code>), feel free to expand the EC2 instance profile - but always ensure that the minimum permissions specified here are included.</p> Add <code>AmazonSSMManagedInstanceCore</code> - connect to your self-hosted runners \u2b50 <p>Session Manager is my favourite way to connect to instances as they do not require bastion hosts or managing SSH keys. I recommend expanding your EC2 instance role with AWS Managed Policy AmazonSSMManagedInstanceCore. When used with Machine Images built on Amazon Linux 2023 and Ubuntu - Session Manager should work out of the box as the SSM Agent is pre-installed \ud83e\udd29</p>"},{"location":"getting-started/advanced-configuration/#network-security","title":"Network Security","text":""},{"location":"getting-started/advanced-configuration/#security-groups","title":"Security Groups","text":"<p>Only allow outbound traffic necessary. Outbound traffic can be further restricted as per Github's prescription self-hosted runner communication.</p>"},{"location":"getting-started/advanced-configuration/#subnets-and-vpc-endpoints","title":"Subnets and VPC Endpoints","text":"<p>We recommend adding VPC endpoints to your VPC's route tables if you prefer to keep AWS service calls off the public internet. The self-hosted runners already make calls to DynamoDB in the background so a Gateway Endpoints will bring those network costs to zero. Furthermore, if you need to pull container images from ECR or access artifacts in S3, consider adding the S3 Gateway Endpoint as well.</p>"},{"location":"getting-started/monitoring-troubleshooting/","title":"Monitoring, Logging, and Troubleshooting","text":"<p>Needs improvement</p> <p>Feel free to raise pull requests to improve this page!</p>"},{"location":"getting-started/monitoring-troubleshooting/#runner-logs","title":"Runner Logs","text":"<p>Looking for specific signals. As per the architecture, the Github Runners and Instances communicate with each other via various \"signals\". You can see the state of specific instances via very specific signals.</p>"},{"location":"getting-started/monitoring-troubleshooting/#accessing-an-instance","title":"Accessing an instance","text":"<p>If something has gone wrong, it might be worth looking at the logs the userdata is producing. Once you have access to the instance - I recommend tailing the user-data logs.</p> <pre><code>tail -f /var/logs/user-data.logs\n</code></pre>"},{"location":"getting-started/monitoring-troubleshooting/#accessing-dynamodb-database","title":"Accessing DynamoDB Database","text":"<p>Add prescription on seeing the dynamodb database. How to look at the underlying data structures.</p>"},{"location":"getting-started/monitoring-troubleshooting/#accessing-the-sqs-queues","title":"Accessing the SQS Queues","text":"<p>Add prescription on how to read the SQS queues.</p>"},{"location":"getting-started/prerequisites/","title":"Prerequisites","text":"<p>Before Quickstart, we need to sort out the inputs for <code>mode: refresh</code> in order to initialize everything. Once done, these inputs are used in <code>.github/workflows/refresh.yml</code> - like so:</p> Example - <code>.github/workflows/refresh.yml</code> <pre><code>jobs:\n  refresh_job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          # IAM User credentials for GH Runner\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          # Github Personal Access Token\n          github-token: ${{ secrets.GH_PAT }}\n          # EC2 Machine Image\n          ami: ami-123\n          # Profile passed on to the EC2 instance for permissions\n          iam-instance-profile: your-instance-profile\n          # Security group assigned to EC2 instances\n          security-group-ids: sg-123\n          # Subnets where EC2s are placed\n          subnet-ids: subnet-123 subnet-456 subnet-789\n          # AWS region (default: us-east-1)\n          aws-region: us-east-1\n          # Injected script, install additional packages here, see Machine Image section below\n          pre-runner-script: |\n            echo \"hello world\"\n</code></pre> <p>Here's your checklist:</p> <ul> <li> GitHub Personal Access Token (PAT)</li> <li> Networking: VPC, Subnet and Security Group</li> <li> IAM User &amp; Permissions for GitHub Actions Workflow</li> <li> IAM Role &amp; Instance Profile for EC2 Instances</li> <li> Machine Image: EC2 AMI image</li> </ul>"},{"location":"getting-started/prerequisites/#1-github-personal-access-token-pat","title":"1. GitHub Personal Access Token (PAT)","text":"<p>A GitHub Personal Access Token (PAT) is required for the action to manage self-hosted runners. This will be used for the <code>refresh</code> mode creates temporary tokens to handles runner registration with GitHub.</p> <ol> <li>Create the PAT with <code>repo</code> scope</li> <li>Store as a secret named <code>GH_PAT</code>. We access this as <code>github-token: ${{ secrets.GH_PAT }}</code></li> </ol> <pre><code># used here ...\n      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          github-token: ${{ secrets.GH_PAT }} # &lt;----\n</code></pre>"},{"location":"getting-started/prerequisites/#2-networking","title":"2. Networking","text":"<p>For this action to work, we need an available VPC, Subnet and Security group</p> <ul> <li>VPC: Create a new VPC or use an existing one</li> <li>Subnet: Ensure that the subnet can reach the internet (public subnet, or private subnet with a NAT Gateway)</li> <li>Security Group: When getting started, I recommend creating a permissive security groups and narrow down from there. If you need more restriction, see github's recommendation</li> </ul> <pre><code># used here ...\n      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          security-group-ids: sg-123 # &lt;-----\n          subnet-ids: subnet-123 subnet-456 subnet-789 # &lt;-----\n</code></pre> <p>Recommendation: 1 VPC with a subnet per AWS Region's availability zones</p> <p>While atleast one subnet is required. I recommend creating and inputting a subnet for each of the availability zones to ensure that for however many instances is required for a workflow, AWS can quickly and cheaply provide them.</p> <p>Note: Accessibility to DynamoDB</p> <p>The created instances polls the dynamodb service for various functionalities (safe self-termination, liveliness, etc.). When hardening access, just ensure that dynamodb is still accessible for the instances.</p>"},{"location":"getting-started/prerequisites/#3-iam-user-for-github-actions-workflow","title":"3. IAM User for GitHub Actions Workflow","text":"<p>For the GitHub Actions workflow to interact with your AWS account (to manage EC2 instances, DynamoDB, SQS, Launch Templates, etc.), it needs an IAM identity with appropriate permissions. We will be using aws-actions/configure-aws-credentials</p> <p>To quickly get started, we will be using an IAM User with credentials stored secrets with names: <code>AWS_ACCESS_KEY_ID</code> &amp; <code>AWS_SECRET_ACCESS_KEY</code>. Consider using OIDC for further hardening.</p> Policy for GitHub Actions Workflow <p>This policy grants the GitHub Actions workflow the necessary permissions. The <code>iam:PassRole</code> permission is crucial for allowing the workflow to pass an IAM role to the EC2 instances it creates (the EC2 Instance Profile). <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:CreateFleet\",\n        \"ec2:RunInstances\",\n        \"ec2:TerminateInstances\",\n        \"ec2:CreateTags\",\n        \"ec2:*LaunchTemplate*\",\n        \"ec2:Describe*\"\n      ],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"iam:PassRole\"],\n      \"Resource\": \"arn:aws:iam::*:role/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"dynamodb:*Item\",\n        \"dynamodb:Query\",\n        \"dynamodb:Scan\",\n        \"dynamodb:DescribeTable\",\n        \"dynamodb:CreateTable\",\n        \"dynamodb:ListTables\"\n      ],\n      \"Resource\": \"arn:aws:dynamodb:*:*:table/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"sqs:*Queue*\", \"sqs:*Message\"],\n      \"Resource\": \"arn:aws:sqs:*:*:*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"iam:CreateServiceLinkedRole\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"iam:AWSServiceName\": [\"spotfleet.amazonaws.com\", \"ec2.amazonaws.com\"]\n        }\n      }\n    }\n  ]\n}\n</code></pre></p> <pre><code># used here ...\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }} # &lt;---\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} # &lt;---\n          aws-region: us-east-1\n      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          # ...\n</code></pre>"},{"location":"getting-started/prerequisites/#4-iam-role-instance-profile-for-ec2-instances","title":"4. IAM Role &amp; Instance Profile for EC2 Instances","text":"<p>The EC2 instances launched by this action need their own set of permissions to perform tasks (see: instance agent) These permissions are granted via an IAM Role -&gt; attached to the instances through an EC2 Instance Profile.</p> <p>To quickly get started, use the AWS console to create an IAM Role. When you select \u201cEC2\u201d as the trusted service during role creation, the console automagically generates a matching EC2 Instance Profile (with the same name). Once the role and its instance profile exist, attach the following policies </p> Policy for EC2 Instance Profile <p>This policy allows the EC2 instances to interact with DynamoDB and terminate themselves if they carry a specific tag. <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\"dynamodb:*Item\"],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    },\n    {\n      \"Action\": \"ec2:TerminateInstances\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"ec2:ResourceTag/AllowSelfTermination\": \"true\"\n        }\n      },\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre></p> <pre><code># used here...\n      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          iam-instance-profile: your-instance-profile # &lt;---\n</code></pre> <p>The example policies above are provided as a guide - See Advanced Configuration for hardening!</p>"},{"location":"getting-started/prerequisites/#5-machine-image","title":"5. Machine Image","text":""},{"location":"getting-started/prerequisites/#quickstart-recommendation-runs-on-community-ami-images","title":"Quickstart Recommendation: Runs On Community AMI Images","text":"<p>To get up and running, I recommend using run-on's AWS AMI images. This will get you started quickly as they graciously keep an EC2-compatible machine image up to date with the packages used by the official Github Runner images.</p> <pre><code># used here...\n      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          ami: ami-123 # &lt;---\n</code></pre>"},{"location":"getting-started/prerequisites/#other-images-amazon-linuxubuntu","title":"Other Images (Amazon Linux/Ubuntu)","text":"<p>If you want to get started with other machine images, see prescriptions at Advanced Configuration - custom AMIs and pre runner scripts</p>"},{"location":"getting-started/quickstart/","title":"QuickStart","text":"<p>Once you've completed the Prerequisites, you can set up a basic workflow as illustrated below:</p> <p></p> <p>For details on fine-tuning timeouts and other settings, please refer to the Advanced Configuration guide.</p>"},{"location":"getting-started/quickstart/#lets-get-started-follow-in-order","title":"Let's get started! (follow in order)","text":"<ol> <li>Create the <code>refresh.yml</code> file (see below) with inputs from Prerequisites.</li> <li>Manually run the refresh workflow with workflow dispatch (or wait for the cron to execute)</li> <li>If refresh is OK -&gt; create <code>ci.yml</code> and push </li> </ol> <p>Resource Pooling - reruns are faster </p> <p>On your first run, your provision step may take 30 to 90s due to OS startup and any pre-runner-scripts (if any). But, as we reuse runners between workflows, the instances are picked up from the pool instead </p>"},{"location":"getting-started/quickstart/#files-to-create","title":"Files to Create","text":"Create - <code>.github/workflows/refresh.yml</code> <pre><code># in .github/workflows/refresh.yml\nname: Refresh Workflow\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"*/15 * * * *\"\n\njobs:\n  refresh_job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Refresh Mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: refresh\n          github-token: ${{ secrets.GH_PAT }}\n          ami: ami-123\n          iam-instance-profile: your-instance-profile\n          security-group-ids: sg-123\n          subnet-ids: subnet-123 subnet-456 subnet-789\n          aws-region: us-east-1\n</code></pre> Create - <code>.github/workflows/ci.yml</code> <pre><code># in .github/workflows/ci.yml\nname: CI\n\non:\n  pull_request:\n    branches: [\"*\"]\n  push:\n    branches: [main]\n\njobs:\n  provision: ### Picks up resources from pool OR creates if not available\n    runs-on: ubuntu-latest\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Provision\n        id: provision_step\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: provision\n          instance-count: 2\n\n  lint:\n    needs: provision\n    runs-on: ${{ github.run_id }}\n    steps:\n      - name: run hello matrix\n        run: |\n          echo \"hello matrix\"\n          pwd\n          aws --version\n          docker --version\n          sleep 20\n\n  test:\n    needs: provision\n    runs-on: ${{ github.run_id }}\n    steps:\n      - name: run hello matrix\n        run: |\n          echo \"hello matrix\"\n          pwd\n          aws --version\n          docker --version\n          sleep 20\n\n  release: ### Releases used resources to pool\n    needs:\n      - provision\n      - lint\n      - test\n    runs-on: ubuntu-latest\n    if: ${{ always() }} # always release resources for reuse\n    steps:\n      - name: Configure AWS Credentials\n        uses: aws-actions/configure-aws-credentials@main\n        with:\n          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}\n          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n          aws-region: us-east-1\n      - name: Release\n        id: release_mode\n        uses: fleet-actions/ec2-runner-pool-manager@main\n        with:\n          mode: release\n</code></pre>"}]}